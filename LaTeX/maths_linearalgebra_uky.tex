%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Notes from Avinash Sathaye}
https://http://www.ms.uky.edu/~sohum/ma162/fa\_09/lectures/
\end{frame}

\begin{frame}

  \frametitle{Basic Definitions.} 
  \begin{itemize}%[<+-| alert@+>]
  \item 
    A linear function of one variable $x$ is a function $f(x)=mx+c$ where $m,c$ are constants.
    Its graph is a straight line, hence it is called linear.
    
    {\bf Example:} $\displaystyle f(x) = 3x+4$.
   %\pause 
   
  \item A linear function of two variables $x,y$ is of the form $f(x,y)=ax+by+c$. Its graph is a plane in three space.
  {\bf Example:} $\displaystyle f(x,y) = 3x+4y+5$.

    %\pause
  \item 
    A natural generalization is a linear function of $n$ variables 
    $\displaystyle f(x_1,x_2,\cdots ,x_n)= a_1x_1+a_2x_2+\cdots a_nx_n+b$
    where $a_1,a_2,\cdots , a_n,b$ are constants.
    
  {\bf Example:} $\displaystyle f(x,y,z) = 3x+4y-5z+7$.  
    %\pause
  \item 
   These functions are useful in many applications.
   %\pause
   
   \item 
   What are examples of functions which are {\bf not} linear?
    {\bf Example:} $\displaystyle f(x)=x^2+3x, g(x,y)=x^3-y^3, h(x,y,z)=xy+yz+zx $.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Real Life functions}
  Here are some examples of real life functions which behave like linear functions.
%\pause     
 \begin{itemize}%[<+-| alert@+>]

\item {\bf Distance traveled}
If the time interval is short or if an object is moving without acceleration, then $s=at+b$ 
describes the distance traveled at time $t$. 
The coefficient $a$ is the constant {\bf velocity.} Its sign 
describes if the object is moving away or coming closer.
%\pause 
\item {\bf Revenue, Cost  and Profit Function.} If $x$ is the number of units sold or
manufactured, then  we have three natural functions associated with it.
%\pause 
\item The cost function is $C(x)=cx+f$ where $c$ is the production cost per unit 
 and $f$ is the fixed  cost.
%\pause 
\item The revenue function is $R(x) = px$ where $p$ is the  price per unit.
%\pause 
 \item
 The profit function:  $P(x)=R(x)-C(x)$ or $(p-c)x -f $.
\end{itemize}

\end{frame}

 

 
 
 

\begin{frame}
  \frametitle{Lines or the linear functions of one variable.}
 
 
 \mbl{We now review how to study the properties of a linear function 
  of one variable $x$ by known geometric properties of its graph, the line.}

  %\pause

  
    \begin{itemize}
    \item {\bf Plane coordinates}
    Recall that points in the plane are pairs of numbers $(x,y)$, these are the $x$ and $y$ 
    coordinates respectively.
    
    A point named $P$ with coordinates $(2,3)$ can be denoted as $P(2,3)$.
    
      
      %\pause
      
    \item {\bf Distance Formula.}
    Recall that the distance between two points $P(a_1,b_1)$ and $Q(a_2,b_2)$ is
    given by the formula
    $$d(P,Q) = \sqrt{(a_2-a_1)^2+ (b_2-b_1)^2}.$$
    
    %\pause
    {\bf Example.} The distance between $P(2,3), Q(-1,7)$ is:
    
    $$d(P,Q) = \sqrt{(-1-2)^2+(7-3)^2}=\sqrt{9+16}=5.$$
    
    \end{itemize}
    
      

\end{frame}

\begin{frame}
  \frametitle{Graph of a function.}
  
 
    
    The graph of a function $y=f(x)$ consists of all points $P(x,y)$ for which $y=f(x)$.
    
    %\pause
    
    {\bf Example.} The graphs of the lines 
    $$y=\frac{2x}{3}, y=1-\frac{2x}{3}, \mbox{ and } y=3 $$
    are shown on the same axes below.
    
    %\pause
    
  {\centerline{\pict{1.5}{1.5}{lines.jpg}}}

  
\end{frame}


\begin{frame}
  \frametitle{More about lines}
\begin{itemize}

\item As the graph of a linear function, a line has the equation $y=mx+c$.

%\pause
\item A vertical line does not appear as the graph of linear function. Indeed, it cannot 
be the graph of {\bf any function.}

%\pause

A vertical line is described by an equation of the form $x=p$ where $p$ is a constant.

%\pause

\item We may combine these two cases and say that the
\mbl{general equation of a line} in the 
plane is of the form:

$$ax+by+c=0$$
where at least one of $a,b$ are non zero.



\end{itemize}
  
\end{frame}

\begin{frame}
 
    \frametitle{Recognizing a Line.} 


    \begin{enumerate}%[<+-| alert@+>]
 \item  Given a line $ax+by+c=0$, if $b\neq 0$, then it is not vertical.
 
 \mbl<Indeed,> we can rewrite it as 
 $$by=-ax-c \mbox{ or } y=\frac{-a}{b}x+\frac{-c}{b}.$$
 
 %\pause
 {\bf Example.}
 
 The line $2x-3y+5=0$ is rewritten as $\displaystyle y=\frac{-2}{-3}x+\frac{-5}{-3}$
 or $\displaystyle y=\frac{2}{3}x+\frac{5}{3}.$
 
 \item The slope of the general line $ax+by+c=0$ is $m=-\frac{a}{b}$ when $b\neq 0$. 
 
 \mbl{If $b=0$} then the line is vertical and slope is infinite. We sometimes allow an equation 
 $m=\infty$ to express this idea.
 
 
 \item Positive slope describes a line rising to the right, negative slope describes a line falling to the right, 
 zero slope gives a horizontal line.
 
 Infinite slope gives a vertical line as already explained.

 
 
    \end{enumerate}
 
\end{frame}

\begin{frame}
  \frametitle{Equations of Lines.}
  
 \begin{itemize}%[<+-| alert@+>]
 \item Given two distinct points in the plane, there is a unique line joining them.
 
 If the two points are $(a_1,b_1)$ and $(a_2,b_2)$, then the slope of this line is:
 $$m=\frac{b_2-b_1}{a_2-a_1}.$$
 
 {\bf  Example:} The slope of the line joining $(2,3)$ and $(-1,6)$ is:
 %\pause
 $$\frac{6-3}{-1-2} = -1.\leqno{\mbox{Formula for Slope }}$$
\end{itemize}
\end{frame}
\begin{frame}
  \frametitle{More Equations of Lines.}
  \begin{itemize}%[<+-| alert@+>]
  
 \item The equation of a line with a given slope $m$ and passing through a point $(a,b)$ is:
 
 $$y-b = m(x-a) \leqno{\mbox{\bf ~~Point Slope Formula}}.$$
  {\bf  Example:} The slope of the line joining $(2,3)$ and $(-1,6)$ is:
  %\pause
  We use the calculated slope $-1$ and the point $(2,3)$ to get:
  $$y-3 = -1(x-2) \mbox{ simplifis to } y = -x+5 \mbox{ or } x+y-5=0.$$
 
 \item \textcolor{blue}{As expected, all formulas need a special handling when the slope 
 becomes infinite, 
 i.e. when the line is vertical.
 The reader should make this adjustment as necessary.}
 
 
 \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Intercepts of Lines.}
  \begin{itemize}%[<+-| alert@+>]
  
 \item The intercept of a line refers to its intersection with an axis, when defined.
 
 \item Thus, the $x$-intercept of a line $ax+by+c=0$ is given by its intersection with the $x$-axis 
 (i.e. $y=0$) and is clearly equal to $\displaystyle -\frac{c}{a}$.
 
 When $a=0$, this is infinite if $c\neq 0$ and undefined when $c=0$, 
 i.e. when the  line is the whole $x$-axis.
 
 
 \item Similarly, the $y$-intercept of a line $ax+by+c=0$ is given by its intersection with 
 the $y$-axis 
 (i.e. $x=0$) and is clearly equal to $\displaystyle -\frac{c}{b}$.
 
 When $b=0$, this is infinite if $c\neq 0$ and undefined when $c=0$, 
 i.e. when the  line is the whole $y$-axis.
 
 \item Note that for the line equation $y=mx+c$, the $y$-intercept is $c$.
 
 \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Intercepts form of Lines.}
  \begin{itemize}%[<+-| alert@+>]
  
 \item {\bf Example:} What are the $x$ and $y$ intercepts of $2x-3y+5=0$?
 
 %\pause
 {\bf Answer:}
 The $x$-intercept is $-\frac{5}{2}$ and the $y$-intercept is $\frac{5}{3}$.
 
 \item Sometimes, it is desirable to get the equation of a line with given $x$-intercept $p$ 
 and $y$-intercept $q$.
 
 A nice formula is:
 $$\frac{x}{p}+\frac{y}{q} = 1 \leqno{\mbox{ \bf Intercept Form}}.$$
 
 {\bf Example:} What is the equations of a line with $x$-intercept $-3$ and $y$-intercept $2$?
 %\pause
 {\bf Answer:} 
 $$\frac{x}{-3}+\frac{y}{2}=1 \mbox{ which simplifies to } 2x-3y+6=0.$$
 
 
 \item As before, special cases when $p$ or $q$ are zero must  be handled separately.
 These correspond to lines through the origin and it is best to use alternate formulas.
 
 \end{itemize}

\end{frame}

\begin{frame} %2

  \frametitle{Review: Basic Definitions.} 
  \begin{itemize}%[<+-| alert@+>]
  \item 
    A linear function of one variable $x$ is a function $f(x)=mx+c$ where $m,c$ are constants.
    Its graph is a straight line, hence it is called linear.
    
    {\bf Example:} $\displaystyle f(x) = 3x+4$.
   %\pause 
   
  \item A linear function of two variables $x,y$ is of the form $f(x,y)=ax+by+c$. Its graph is a plane in three space.
  {\bf Example:} $\displaystyle f(x,y) = 3x+4y+5$.

    %\pause
  \item 
    A natural generalization is a linear function of $n$ variables 
    $\displaystyle f(x_1,x_2,\cdots ,x_n)= a_1x_1+a_2x_2+\cdots a_nx_n+b$
    where $a_1,a_2,\cdots , a_n,b$ are constants.
    
  {\bf Example:} $\displaystyle f(x,y,z) = 3x+4y-5z+7$.  
    %\pause
  \item 
   These functions are useful in many applications.
   %\pause
   
   \item 
   What are examples of functions which are {\bf not} linear?
   
    {\bf Example:} $\displaystyle f(x)=x^2+3x, g(x,y)=x^3-y^3, h(x,y,z)=xy+yz+zx $.
  \end{itemize}
\end{frame}

\begin{frame} %3
  \frametitle{Some interesting Linear  Functions}
  Here are some examples of real life functions which behave like linear functions.
%\pause     
 \begin{itemize}%[<+-| alert@+>]
\item {\bf Tax Calculations.}
Typically, tax calculation on an income of $x$ dollars is a linear function when $x$ lies in a
specific tax bracket.
The function formula, however, changes with tax brackets. This is a good example of a step function 
which is defined by different formulas in different ranges of $x$ values.

A typical formula looks like 
$$t(x) = f+r(x-b)$$
where $x$ is assumed to be at least $b$, $f$ is the fixed tax for income $b$ and $r$ is the tax 
on each dollar earned above $b$.
\end{itemize}

\end{frame}
\begin{frame}%4
  \frametitle{Tax Calculations continued.}

{\bf Example:} What is the tax on \$$49,000$, if tax is charged at $6$\% on all income 
above \$$15,000$?


%\pause

{\bf Answer:} Tax is $0.06 (49000-15000)=2040$ dollars.

{\bf Example continued:}
What is the tax if amounts above \$$50,000$ are charged at the rate of $7$\% 
and the income is \$$60,000$?

%\pause
{\bf Answer:}

Tax on \$$50,000$ by the  first formula is $f=0.06(50000-15000)=2100$.
Tax for the excess of \$$(60000-50000)$ or \$$10,000$ is $0.07(10000) = 700$.

Thus, the net tax is $2100+700=2800$ dollars.

\end{frame}
\begin{frame}%5
  \frametitle{Depreciation.}
  \begin{itemize}%[<+-| alert@+>]

\item {\bf Depreciation.} If an initial value $b$ is to be depreciated to value zero in $d$ years, then the depreciated value 
for any $t$ between $0$ and $d$ is given by the formula:
$$ v(t) = b - \frac{b}{d}t.$$

{\bf Example:} If a car worth \$$45,000$ is to be depreciated to zero in $6$ years, what is its value after 
$4$ years.

%\pause
{\bf Answer:}
Here $b=45000, d=6$ and $t=4$.

So the formula gives $45000-\frac{45000}{6}4 = 15000$.
\end{itemize}
\end{frame}

\begin{frame}%6
  \frametitle{Financial Functions}
%\pause     
 \begin{itemize}%[<+-| alert@+>]
\item {\bf Review of Business Functions.} If $x$ is the number of units sold or manufactured, then 
 we have three natural functions associated with it.
%\pause 
\item The cost function is $C(x)=cx+f$ where $c$ is the production cost per unit 
 and $f$ is the fixed 
 cost.
%\pause 
\item The revenue function is $R(x) = px$ where $p$ is the selling price of a unit.
%\pause 
 \item
 The profit function is then given by $\displaystyle P(x)=R(x)-C(x) =(p-c)x -f $.
\end{itemize}

\end{frame}


\begin{frame}%7
  \frametitle{Breakeven Analysis}
 
  \alert{We now describe how to determine the minimum production level which guarantees no 
  net loss.}
  
    \begin{itemize}%[<+-| alert@+>]
    \item {\bf Breakeven Production.}
    A production level $x$ is said to be break even if the net profit is zero, i.e.
    $$P(x) = R(x) - C(x) = (p-c)x -f = 0.$$
    
    	
      
      %\pause
      
    \item {\bf Another viewpoint.}
    The breakeven production can be interpreted alternatively as follows.
    
    %\pause
    Consider the graphs of the revenue function $R(x)$ and the cost function $C(x)$ plotted 
    on the same axes. Both graphs are lines.
    
    Then the break even production is simply the $x$-coordinate of the common point. 
    \end{itemize}

\end{frame}

\begin{frame}%8
  \frametitle{Example of Breakeven Analysis.}
   
    {\bf Example.} Suppose that a certain toy can be manufactured for \$$3.5$ each and 
    the cost of maintaining the workshop is \$$21,000$ per month.
    
    If the toy is sold for \$$5$, determine the monthly breakeven production.
    
    %\pause
    
  {\bf Answer:}
  Let $x$ denote the monthly production.
  Then the revenue function is $R(x)=5x$.
  %\pause 
  The cost function is $C(x)=3.5x+21000$.
  
  We find the common point of the graphs of $y=R(x)$ and $y=C(x)$, thus we solve:
  
  $$5x=3.5x+20000 \mbox{ or } 1.5x = 21000.$$
  
  The breakeven production is $\disp x=\frac{21000}{1.5} = 14000$.
  
  
\end{frame}


\begin{frame}
  \frametitle{Intersecting lines}
The above example motivates a review of techniques to intersect two lines. 

We consider two linear equations $ax+by=c$ and $px+qy=r$ and discuss their common solutions.
%\pause
 \begin{itemize}%[<+-| alert@+>]
 \item {\bf Substitution method.} 
%\pause
The most familiar method is to solve one of the equations for $y$ and substitute 
the solution in the other  to find the $x$-coordinate of the common point.

Then use it and one of the equations to find $y$.
%\pause
{\bf Example:} Solve
$$E1:~~3x-y=5 \mbox{ and } E2:~~ 2x+3y = 7.$$

%\pause
{\bf Answer:} 
Solving E1 for $y$, we get $y=3x-5$.
%\pause
Substitution in E2 gives: $2x+3(3x-5)=7$ or $11x = 22$.
This gives $x=2$
%\pause 
 and finally  $y=3x-5 = 3(2)-5 = 1$. 

\end{itemize}
  
\end{frame}




\begin{frame}
 
    \frametitle{Continued Intersections of Lines.} 


    \begin{itemize}%[<+-| alert@+>]
    
 \item Thus, the intersection of the lines 
 $$E1:~~3x-y=5 \mbox{ and } E2:~~ 2x+3y = 7$$
 is given by  $x=2,y=1$.
    
 \item {\bf Comment.} 
 \textcolor{blue}{Though easy to understand, the above method is not always the most 
 efficient and it is helpful to learn 
other techniques as well.}
%\pause

\item {\bf Cramer's Rule.}
This technique lets us write down the answer to a system of two linear equations in 
two variables by a formula.

It can also generalize to several equations in several variables.

    \end{itemize}
 
\end{frame}




\begin{frame}
  \frametitle{Determinants.}
  
 \begin{itemize}%[<+-| alert@+>]
 \item {\bf Determinant.}
 Given a $2 \times 2 $ matrix $A = \bmatr{2}{a & b\\c& d}$ we define its determinant:
 $$\det(A) = ad-bc.$$
 
  Sometimes this is also written as 
 $$\datr{2}{a & b \\c & d}=ad-bc.$$
 %\pause


 {\bf  Example:} $$\datr{2}{5 & -3 \\ 2 & 4} = (5)(4)-(-3)(2)=26.$$
 
 \end{itemize}
\end{frame}



 
\begin{frame}
  \frametitle{Cramer's Rule.}
  \begin{itemize}%[<+-| alert@+>]
  
 \item Now we describe the Cramer's rule for solving two equations in two variables:
 $$E1: ax+by=c,~~ E2: px+qy=r.$$
 
 %\pause
 
 
 \item Calculate the determinants:
 $$\Delta = \datr{2}{a & b \\ p& q},~ \Delta_x=\datr{2}{c & b \\ r& q},~
 \Delta_y=\datr{2}{a & c \\ p& r}.$$
 
 %\pause
 \item Then the answers are: 
 $$x = \frac{\Delta_x}{\Delta} \mbox{ and } y = \frac{\Delta_y}{\Delta}.$$
 
 
 
 \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Cramer's Rule: continued.}
  \begin{itemize}%[<+-| alert@+>]
 
 \item Recall the answers: 
 $$x = \frac{\Delta_x}{\Delta} \mbox{ and } y = \frac{\Delta_y}{\Delta}.$$
 
 %\pause
 \item These are undefined when $\Delta=0$ and in that case we have:
 \begin{itemize}
 \item If one of $\Delta_x,\Delta_y$ is non zero, then the system has no solution.
 %\pause
 \item If all three determinants are zero then one equation is a multiple of another and we could have 
 infinitely many solutions \textcolor{blue}{\bf unless} we happen to have an equation where all coefficients of variables are zero and some right hand side is non zero!
 
 \end{itemize}
 
 
 \end{itemize}

\end{frame}



\begin{frame}
  \frametitle{Example of Cramer's Rule.}
  \begin{itemize}%[<+-| alert@+>]
  
 \item We redo the old example.
 
 {\bf Example:} Solve
$$E1:~~3x-y=5 \mbox{ and } E2:~~ 2x+3y = 7.$$
 
 %\pause
 {\bf Answer:}
We see that 
$$\Delta = \datr{2}{3 & -1 \\ 2 & 3},~~\Delta_x = \datr{2}{5 & -1 \\ 7 & 3},~~
\Delta_y=\datr{2}{3 & 5 \\ 2 & 7}.$$
%\pause
\item Evaluation gives:
$$
\begin{array}{lllll}
\Delta & = & (3)(3)-(-1)(2)& = & 11\\
 \Delta_x& = & (5)(3)-(-1)(7)& = & 22\\
 \Delta_y& = & (3)(7)-(5)(2) & = &  11
 \end{array}
 $$
%\pause
\item So the answers are: 
$$x=\frac{22}{11}=2,~~y=\frac{11}{11}=1$$
as alreday known!

 
\end{itemize}

\end{frame}
\begin{frame} %2

  \frametitle{Chapter 2.1, 2.2 Solving Linear Equations.} 
  \begin{itemize}%[<+-| alert@+>]
  \item 
    As we have seen before, several practical problems lead to
    formulation and solution of linear equations.
    First we see several examples of this
    (compare exercises in Chap. 2.1.)
    
    
   %\pause 
   
 
  \item 
    \textcolor{blue}{Production constraints:}
    Suppose we have a $500$ acre farm.
    Suppose that the production of corn needs \$$42$ per acre while wheat
    requires \$$30$ (EX. 2.1.15).

    Suppose we have \$$18,600$ available
    and we decide to plant in $x$ acres of corn and $y$ acres of wheat.

    What is the mathematical setup of the problem? 
    
  
    %\pause
  \item 
   The constraint due to the farm size is:
   $$x+y = 500.$$
   %\pause
   The constraint due to the available money is:
   $$42x+30y = 18600.$$
  
   
   
  \end{itemize}
\end{frame}

%2

\begin{frame}%3
  \frametitle{Samples continued.}
  \begin{itemize}%[<+-| alert@+>]
 
\item
Solving the two equations
$$x+y = 500 \mbox{ and } 42x+30y = 18600 $$
together gives the answer:
   $$x=300,~ y=200.$$
\item   
   Thus we {\bf report the answer:} plant $300$ acres of corn and $200$
   acres of wheat!
\end{itemize}
\end{frame}

%3

\begin{frame}%4
  \frametitle{Investment Example.}
  \begin{itemize}%[<+-| alert@+>]
 
\item \textcolor{blue}{Investment:}
Two investments yield $8$ and $10$ percent annually.
If a total of \$$30,000$ is invested and yield is  \$$2640$ per year,
how is the fund split between the two investments?

(Compare Chap. 2.1.18)
%\pause 
\item We assume that amounts in the two funds  (with yields of
$8$\% and $10$\% ) are $x$ and $y$ dollars
respectively. Then we get the two constraints:
%\pause
$$x+y=30000 \leqno{\mbox{Net value matched}}.$$
%\pause
$$0.08x+0.10y=2640 \leqno{\mbox{Net return matched}}.$$
%\pause
{\bf Answer:} Solution gives $x=18,000$ and $y=12,000$.
\end{itemize}
\end{frame}

%4

\begin{frame}%5
  \frametitle{Diet Example.}
  \begin{itemize}%[<+-| alert@+>]
 
\item Three kinds of food products give different percentages
of proteins, carbohydrates and iron.
\item Name the three products $A,B,C$.
Then we are given the contents per ounce of the products.
\begin{itemize}
\item Food product $A$ provides $10$\% of protein, $10$\%  of
carbohydrates and $5$\% of iron needed daily.
\item Food product $B$ provides $6$\% of protein, $12$\%  of
carbohydrates and $4$\% of iron needed daily.
\item Food product $C$ provides $8$\% of protein, $6$\%  of
carbohydrates and $12$\% of iron needed daily.
\end{itemize}

Calculate how much of each type of food should be eaten to get
the $100$\% RDA of each food. (Compare Chap. 2.1.25).



\end{itemize}
\end{frame}

%5

\begin{frame}%6
  \frametitle{Diet Example continued.}
  \begin{itemize}%[<+-| alert@+>]
\item Set $x,y,z$ respectively the number of ounces of each of the three products. 
\item We get three equations:
$$10x+6y+8z=100 \leqno{\mbox{Protein requirement}}.$$
$$10x+12y+6z=100 \leqno{\mbox{Carbohydrate requirement}}.$$
$$5x+4y+12z=100 \leqno{\mbox{Iron requirement}}.$$

\item Solution yields:
$$x=4,y=2,z=6.$$

\item So we recommend that we eat $4$ ounces of product $A$, $2$ ounces
of product $B$ and $6$ ounces of product $C$.
\end{itemize}
\end{frame}

%6

\begin{frame}%7
  \frametitle{Real Life Situations.}
  \begin{itemize}%[<+-| alert@+>]
 
\item \textcolor{blue}{Point to note: }In real life problems, we
shall find that {\bf we don't get or want} exact equations, but rather
inequalities.

Also, there is usually some payoff function we are trying to maximize
(or some net cost function we are trying to minimize).

\item This is often accomplished by converting the inequalities to equations
by assigning variable names to the difference between the two sides of the
inequalities.

We will get many possible solutions and develop a method to optimize our
payoff (or cost) function. This is the topic of Simplex algorithm coming
up later.


\end{itemize}
\end{frame}

%7

\begin{frame}%8
  \frametitle{The plan of action.}
  \begin{itemize}%[<+-| alert@+>]
 
\item We note that we know everything about how to solve a system of
equations in a single variable.

We also have learnt several techniques to solve two (or more) equations
in two variables.

\item For several equations in several variables, the idea is to extend
the old methods to {\bf systematically eliminate} one variable at a time
and get down to a single variable equation.

\item We begin by an example of manipulating equations and then switch
over to a convenient method of manipulating the matrix of coefficients,
thus avoiding the unnecessary repetition of variable names.

\item This is the method of Gauss-Jordan elimination.

\end{itemize}
\end{frame}

%8

\begin{frame}%9
  \frametitle{Simple Example.}
  \begin{itemize}%[<+-| alert@+>]
 
\item Let us redo the old example of intersecting lines with the new
viewpoint.

{\bf Example:} Solve
$$E1:~~3x-y=5 \mbox{ and } E2:~~ 2x+3y = 7.$$

\item We shall form the matrix of its coefficients with the variables
mentioned on the top.

$$\left[
\begin{array}{rr|l}
x & y & RHS \\
3 & -1 & 5 \\
2 & 3 & 7 \\
\end{array}
\right]
$$

\item Here $RHS$ stands for the right hand side and the vertical bar
denotes the equality signs.

\item We wish to get rid of one of the variables.

\end{itemize}
\end{frame}

%9

\begin{frame}%10
  \frametitle{Comtinued Example.}
  \begin{itemize}%[<+-| alert@+>]

\item
As a good practice, we eliminate the first one in order, namely $x$.

\item A little thought says that $E3=E2-\frac{2}{3}E1$ will give us a new
equation $E3:~~\frac{11}{3}y = \frac{11}{3}$.

We make a matrix for the new system of $E1,E3$.

\item
$$\left[
\begin{array}{rr|l}
x & y & RHS \\
3 & -1 & 5 \\
2 & 3 & 7 \\
\end{array}
\right]
~\Rightarrow ~
\left[
\begin{array}{rr|l}
x & y & RHS \\
3 & -1 & 5 \\
0 & \frac{11}{3} & \frac{11}{3} \\
\end{array}
\right]
$$

\item Clearly we can divide the bottom equation by $\frac{11}{3}$ and
get ourselves a new matrix:

\item
$$\left[
\begin{array}{rr|l}
x & y & RHS \\
3 & -1 & 5 \\
0 & 1 & 1 \\
\end{array}
\right]
$$

\end{itemize}
\end{frame}
%10

\begin{frame}%11
  \frametitle{Back substitution.}
  \begin{itemize}%[<+-| alert@+>]
\item The current form of our coefficient matrix is said to be in {\bf Row Echelon Form}
or {\bf REF} for short.

\item It is well suited to solve the last equation for $y$ and then
using that answer, to solve the first for $x$.

\item Typically, we now go back to writing out the current equations:
$$3x-y=5, y=1.$$

\item Using the $y$-value from the second equation, we deduce $3x-1=5$ or
$3x=6$, i.e. $x=2$.

This work is called {\bf back substitution.}

\end{itemize}
\end{frame}

%11

\begin{frame}%12
  \frametitle{A Bigger Example.}
  \begin{itemize}%[<+-| alert@+>]
 
\item {\bf Compare 2.2.4.}
Start with
$$E1:~~ 3x_1+2x_2 = 0,~~ E2:~~  
x_1-x_2+2x_3 = 4,~~ E3:~~  2x_2-3x_3 = 5.$$

\item We at once write the coefficient matrix:
$$\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
3&2&0&0\\\noalign{\medskip}
1&-1&2&4\\\noalign{\medskip}
0&2&-3&5\end {array} \right]
$$

\item Note that $x_1$ is present in the first equation and the second.
It is missing from the third.

We replace the second equation $E2$ by $E2-\frac{1}{3}E1$ to get rid of
it from $E2$.

The new matrix is:
\end{itemize}
\end{frame}

%12
\begin{frame}%13
  \frametitle{Example Continued.}
  \begin{itemize}%[<+-| alert@+>]
\item
$$\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
3&2&0&0\\\noalign{\medskip}
1&-1&2&4\\\noalign{\medskip}
0&2&-3&5\end {array} \right]
~R_2-\frac{1}{3}R_1 ~\Rightarrow~
 \left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
3&2&0&0\\\noalign{\medskip}
0&-5/3&2&4\\\noalign{\medskip}
0&2&-3&5\end {array} \right]
$$
\item We have recorded this operation as $R_2-\frac{1}{3}R_1$. This
means the second row $R_2$ is replaced by $R_2-\frac{1}{3}R_1$.

\item Be sure to write the changed row as the first term of the
expression, always!

\item The first column is now clean! We work on the second column next.

\end{itemize}
\end{frame}
%13
\begin{frame}%14
  \frametitle{The REF.}
  \begin{itemize}%[<+-| alert@+>]

\item Having cleaned $x_1$ column, we also fix the first row with the
idea that we will use it to solve for $x_1$ at the end.

\item In the second column, $x_2$ appears in the second and third rows.
We use the second row entry to clean out the third row entry.

\item The operation used is
$$R_3 - \frac{2}{-\frac{5}{3}}R_2 \mbox{ or } R_3 + \frac{6}{5}R_2.$$

\item The new matrix is:
$$\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
3&2&0&0\\\noalign{\medskip}
0&-5/3&2&4\\\noalign{\medskip}
0&2&-3&5\end {array} \right]
 R_3 + \frac{6}{5}R_2\Rightarrow
\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
3&2&0&0\\\noalign{\medskip}
0&-5/3&2&4\\\noalign{\medskip}
0&0&-3/5&{\frac {49}{5}}\end {array} \right]
$$
\end{itemize}
\end{frame}
%14
\begin{frame}%15
  \frametitle{Final Answer.}
  \begin{itemize}%[<+-| alert@+>]
 
\item Our equations are now ready to be solved by back substitution.
The current form of the matrix is said to be REF (the Row Echelon Form).

\item We solve the third equation for $x_3$, namely $x_3 =
\frac{\frac{49}{5}}{\frac{-3}{5}}=-\frac{49}{3}$.

\item Plug in this value in the second equation and solve for $x_2$.
$$-\frac{5}{3}x_2+2\frac{-49}{3}=4 \mbox{ or } x_2 = -22.$$

\item Use these values in the first equation to solve:
$$3x_1 + 2(-22) =0 \mbox{ or } x_1 = \frac{44}{3}.$$

This gives the final answer:
$$x_1=\frac{44}{3},x_2=-22,x_3=-\frac{49}{3}.$$

\end{itemize}
\end{frame}
%15

\begin{frame}%16
  \frametitle{Conclusion.}
  \begin{itemize}%[<+-| alert@+>]
 
\item In general, we can follow the same process to solve any number of
equations in many variables.

\item In general, our equations will be rearranged such that each
equation has a distinguished variable (called its pivot variable) which
does not appear in lower equations.

There may be some leftover variables called non pivot variables.

\item Our final answer will express the pivot variables in terms of the
non pivot variables, leaving the non pivot variables free to take any
values!  They will be called the {\bf free variables}.

\item Sometimes, we may wipe out all the variables from an equation. In
this case, if the RHS is non zero, then we have an inconsistent equation
and hence no solution. If the whole equation becomes $0=0$, then we
leave it among the last such rows.
\textcolor{blue}{ We study this in detail in the next lecture.}


\end{itemize}
\end{frame}

\begin{frame} %2

  \frametitle{Terminology for  Gauss-Jordan Elimination.}
  We now explain the general Gauss-Jordan Elimination process.
  First, some definitions:
  %\pause
  
  \begin{itemize}%[<+-| alert@+>]

    \item let $M$ be a matrix. In application, we expect it to be the augmented matrix of
    a system of linear equations, however, this is not relevant to our
    definitions.

    Assume that the \mbl{matrix} has $m$ rows and $n$ columns, so it is of
    \mbl{type} $m\times n$.
    
 \item A \mbl{pivot} in a row is the first non zero entry in it. The \mbl{pivot
 position} is the corresponding column number. The \mbl{pivot position
 sequence (p.p. for short)} is the sequence of such pivot positions in
 order.

 If the row is full of zeros, then we declare that it does not have a
 pivot and the pivot position is declared to be $\infty$.

 
 
  \end{itemize}
\end{frame}

%2

\begin{frame}%3
  \frametitle{Examples.}
  \begin{itemize}%[<+-| alert@+>]
 
\item
{\bf Examples.}
 Consider:
 $$
 A= \left[
 \begin{array}{ccc}
 \mrd{2} & 1 & 5 \\
 0 & \mrd{1} & 1 \\
 0 & \mrd{3} & 2\\
 \end{array}\right],~~
 B= \left[
 \begin{array}{ccc}
 0 & \mrd{-1} & 5 \\
 \mrd{1} & 3 & 1 \\
 0 & 0 & \mrd{2}\\
 \end{array}\right],~~
 C= \left[
 \begin{array}{ccc}
 \mrd{2} & 1 & 5 \\
 0 & \mrd{5} & 1 \\
 0 & 0 & 0\\
 \end{array}\right].
 $$

 What are the pivots and the pivot positions?

 %\pause
 {\bf Answer}
 
 For $A$, the p.p. is $(1,2,2)$ with pivots being $2,1,3$ respectively.

 For $B$, the p.p. is $(2,1,3)$ with pivots being $-1,1,2$ respectively.

 For $C$, the p.p. is $(1,2,\infty)$ with pivots being $2,5$ respectively.
 
\end{itemize}
\end{frame}

%3

\begin{frame}%4
  \frametitle{Elementary Operations.}
  \begin{itemize}%[<+-| alert@+>]
 
\item A matrix is said to be in {\bf Row Echelon Form or REF} if its
p.p. is a strictly  increasing sequence.
{\bf For this definition} we shall consider a sequence of $\infty$ to be
a strictly increasing sequence!

\item 
{\bf Note:} In the above examples, $C$ is in REF, while $A,B$ are not.

\item We allow two operations which help us put a matrix in REF.

\item The first is a row swap. Thus, swapping the first and second row
of $B$ produces
$$B= \left[
 \begin{array}{ccc}
 0 & \mrd{-1} & 5 \\
 \mrd{1} & 3 & 1 \\
 0 & 0 & \mrd{2}\\
 \end{array}\right]
 ~\Rightarrow~
 \left[
 \begin{array}{ccc}
 \mrd{1} & 3 & 1 \\
 0 & \mrd{-1} & 5 \\
 0 & 0 &\mrd{2}\\
 \end{array}\right] \mbox{ p.p. is} (1,2,3)$$
The notation for this is \mbl{$R_1\leftrightarrow R_2$}.
%\pause
\end{itemize}

\end{frame}

%4

\begin{frame}%5
  \frametitle{Elementary Operations continued.}
  \begin{itemize}%[<+-| alert@+>]
 
\item The second operation consists of adding some multiple of one row
to another.

For the matrix $A$, the p.p. is $(1,2,2)$ and we need to make the last
row pivot position bigger to get REF.

\item It is easy to see that subtracting $3R_2$ from $R_3$ does the trick.

We shall write this operation as $R_3-3R_2$ with the convention that the
first mentioned row is being replaced!

\item The result is:
$$A= \left[
 \begin{array}{ccc}
 \mrd{2} & 1 & 5 \\
 0 & \mrd{1} & 1 \\
 0 & \mrd{3} & 2\\
 \end{array}\right]
 ~\Rightarrow ~
 \left[
 \begin{array}{ccc}
 \mrd{2} & 1 & 5 \\
 0 & \mrd{1} & 1 \\
 0 & 0 & \mrd{-1}\\
 \end{array}\right].
$$
The new p.p. is $(1,2,3)$ with pivots $(2,1,-1)$.
%\pause


\end{itemize}
\end{frame}

%5

\begin{frame}%6
  \frametitle{Getting to REF}
  \begin{itemize}%[<+-| alert@+>]
 
\item Now we outline the REF procedure in general.
First find the p.p. of the given matrix.

\item Use row swaps as needed to get the pivot positions increasing, but
they may not be strictly increasing yet!

For example, the matrix $A$ above had p.p. $(1,2,2)$.

\item If two successive rows have the same pivot position, use the
earlier row to push the pivot position of the latter row as described
below.

\item In the example of matrix $A$, the pivot in $R_2$ was $1$, while
the pivot in $R_3$ was $3$ in the same column. Call this entry $3$ to be
the target, which needs to become zero!

\item If the target is in $R_i$ and pivot in $R_j$, then
the operation can be described as $R_i - \frac{target}{pivot}R_j$.

Here it becomes $R_3-\frac{3}{1}R_2$ or $R_3-3R_2$.

\end{itemize}
\end{frame}

%6

\begin{frame}%7
  \frametitle{RREF or alternate procedure after REF.}
  \begin{itemize}%[<+-| alert@+>]
 
\item We explained how to make an augmented matrix of a system of linear
equations be in REF. We also explained how we can finish the solution
process by a ``back substitution'' method.

%\pause

Now we explain an alternate procedure which is essential for the upcoming
Simplex algorithm method.

It also has the advantage that the final solution can be simply {\bf
read} from its display, without further manipulations.


\item This form is called {\bf Reduced Row Echelon Form or RREF}. The
book calls it {\bf the row-reduced form.}

\item We shall first illustrate how to reach this RREF and then give its
formal definition.


\end{itemize}
\end{frame}

%7

\begin{frame}%8
  \frametitle{RREF example.}
  \begin{itemize}%[<+-| alert@+>]
 
\item To get RREF, we must first get REF. So, we shall start with the
already worked example:

$$\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
\mrd{3}&2&0&0\\\noalign{\medskip}
0&\mrd{-5/3}&2&4\\\noalign{\medskip}
0&0&\mrd{-3/5}&{\frac {49}{5}}\end {array} \right]
$$

Let us note that the vertical bar separator and the variable names are
for understanding only and not part of calculations at this stage.

\item We start with the pivot in the last row, namely, $-\frac{3}{5}$.
The operation to perform is to make the pivot $1$ by multiplying the row by
a suitable number and then cleaning up all entries above it to zero.



\end{itemize}
\end{frame}

%8

\begin{frame}%9
  \frametitle{RREF Example continued.}
  \begin{itemize}%[<+-| alert@+>]
 
\item  The first operation is denoted as $-\frac{5}{3}R_3$ and gives a new
matrix:
$$\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
\mrd{3}&2&0&0\\\noalign{\medskip}
0&\mrd{-5/3}&2&4\\\noalign{\medskip}
0&0&\mrd{-3/5}&{\frac {49}{5}}\end {array} \right]
~\Rightarrow~
\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
\mrd{3}&\mbl{2}&0&0\\\noalign{\medskip}
0&\mrd{-5/3}&\mbl{2}&4\\\noalign{\medskip}
0&0&\mrd{1}&{-\frac {49}{3}}\end {array} \right]
$$

\item Next we cleanup the entry $2$ above the pivot (now made $1$) and
this is done with $R_2-2R_3$.
\item It yields:
$$
\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
\mrd{3}&\mbl{2}&0&0\\\noalign{\medskip}
0&\mrd{-5/3}&\mbl{2}&4\\\noalign{\medskip}
0&0&\mrd{1}&{-\frac {49}{3}}\end {array} \right]
~\Rightarrow~
\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
\mrd{3}&\mbl{2}&0&0\\\noalign{\medskip}
0&\mrd{-5/3}&0&\frac{110}{3}\\\noalign{\medskip}
0&0&\mrd{1}&{-\frac {49}{3}}\end {array} \right]
$$
%\pause

\end{itemize}
\end{frame}

%9

\begin{frame}%10
  \frametitle{Comtinued Example.}
  \begin{itemize}%[<+-| alert@+>]

\item
Next, we cleanup the entry $2$ in row $1$ column $2$. We use the pivot
$-5/3$ so the operation shall be $R_1-\frac{2}{-5/3}R_2=R_1+\frac{6}{5}R_2$.

\item Thus we get:
$$\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
\mrd{3}&\mbl{2}&0&0\\\noalign{\medskip}
0&\mrd{-5/3}&0&\frac{110}{3}\\\noalign{\medskip}
0&0&\mrd{1}&{-\frac {49}{3}}\end {array} \right]
~\Rightarrow~
\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
\mrd{3}&0&0&44\\\noalign{\medskip}
0&\mrd{-5/3}&0&\frac{110}{3}\\\noalign{\medskip}
0&0&\mrd{1}&{-\frac {49}{3}}\end {array} \right]
$$

\item Finally, we make all pivots $1$, i.e. we make $\frac{1}{3}R_1$ and
$-\frac{3}{5}R_2$.
This produces:
$$
\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
\mrd{3}&0&0&44\\\noalign{\medskip}
0&\mrd{-5/3}&0&\frac{110}{3}\\\noalign{\medskip}
0&0&\mrd{1}&{-\frac {49}{3}}\end {array} \right]
~\Rightarrow~
\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
\mrd{1}&0&0&\frac{44}{3}\\\noalign{\medskip}
0&\mrd{1}&0&-22\\\noalign{\medskip}
0&0&\mrd{1}&{-\frac {49}{3}}\end {array} \right]
$$
%\pause

\end{itemize}
\end{frame}
%10

\begin{frame}%11
  \frametitle{Reading the Answer.}
  \begin{itemize}%[<+-| alert@+>]
\item Recall that we have the final form:
$$\left[ \begin {array}{ccc|c}
x_1 & x_2 & x_3 & RHS\\
1&0&0&\mbl{\frac{44}{3}}\\\noalign{\medskip}
0&1&0&\mbl{-22}\\\noalign{\medskip}
0&0&1&\mbl{-\frac {49}{3}}\end {array} \right]
$$

\item Now we see the main advantage of RREF.
The final solution is clearly visible on the RHS. To read the value of
$x_1$, find the pivot under it and read off the value of RHS, namely
$\frac{44}{3}$.

Similarly, $x_2=-22,x_3=-\frac{49}{3}$.

\item Thus, RREF takes care of the back substitution without writing the
equations again.


\end{itemize}
\end{frame}

%11
\begin{frame}%12
  \frametitle{How to solve equations in RREF.}
  \begin{itemize}%[<+-| alert@+>]
 
\item We now give an example of a system in RREF with infinitely many
solutions.

Consider an augmented matrix:
$$\left[
\begin{array}{cccc|c}
x & y & z & w & RHS\\
\mrd{1} & 0 & 2 & 0 & \mbl{2}\\
0 & \mrd{1} & 3 & 0 & \mbl{5}\\
0 & 0 & 0 & \mrd{1} & \mbl{3}\\
\end{array}
\right]
$$
\item The pivot variables are $x,y,w$ while $z$ is a non pivot variable.
Thus we solve the three equations in order for $x,y,w$ in terms of the
non pivot (or free) variable $z$.

\item Thus, the answer is:
$\displaystyle x=\mbl{2} - \mrd{2z},~ y= \mbl{5} - \mrd{3z},~ w = \mbl{3}.$
Here $z$ is arbitrary.

\item All we did was to move all the $z$ terms to the RHS and then 
read off the solutions as before.
\end{itemize}
\end{frame}
%12

\begin{frame}%13
  \frametitle{RREF defined.}
  \begin{itemize}%[<+-| alert@+>]
\item
An augmented matrix is said to be in RREF if it is in REF and satisfies
the following additional conditions:
\begin{enumerate}
\item The pivot in each row is equal to $1$.
\item All entries in the pivot column below {\bf or above}
the pivot are equal to zero. The book describes this as the pivot
column being a unit column, this being a typical column of the identity
matrix.

\end{enumerate}
\item Of course, it can happen that in RREF, the equations are
inconsistent. This happens when the pivot of some row only appears on
RHS. Such equations have no solutions.
\end{itemize}
\end{frame}
%13
\begin{frame}%14
  \frametitle{Some Treminology.}
  \begin{itemize}%[<+-| alert@+>]

\item The number of pivots in RREF only depends on the starting matrix
and is called its rank.

\item For obvious reasons, rank of a matrix is less than or equal to its
number of rows as well as number of columns.

\item The system either has no solutions (if an inconsistent equation is
present) or a unique solution (if all variables are pivot variables) or
infinitely many, if there is at least one non pivot variable.

This is the so-called $0-1-\infty$ principle of linear algebra.

\end{itemize}
\end{frame}
%14


\begin{frame}%15
  \frametitle{Comments.}
  \begin{itemize}%[<+-| alert@+>]
 
\item We can make some general observations based on the above.

\item Given a system of $m$ equations in $n$ variables, let $r$ be the
rank (i.e. the number of pivots) in the RREF of its augmented matrix.

\item $r\leq \min\{m,n\}$.

\item The system has a unique solution iff $r=n$.

\item The system is consistent (i.e. has at least one solution) iff
$r=m$ and no pivot occurs on the RHS.

\item The number of free (arbitrary) variables is $n-r$.

\item The general solution expresses the pivot variables as suitable
constants plus certain combinations of the non pivot variables.
%\pause
\end{itemize}
\end{frame}

\begin{frame} %2

  \frametitle{Definition of a Matrix.}
 Here we mainly cover
 2.5 in great detail. You should study 2.4 from the examples in the book
 and on WHS.

  
  \begin{itemize}%[<+-| alert@+>]

    \item A matrix is simply a rectangular array of numbers. If $M$ is a
    matrix with $m$ rows and $n$ columns, then we say it is of type
    $m\times n$ and convey the same meaning by saying $M=M_{m\times n}$.

    {\bf Example.}
    $$
    A=\bmatr{2}{1 & 2\\ 3& 4\\0 & -3\\3 & 0 \\}~,
    B=\bmatr{5}{2 & -1 & 3 & 0 & 1}~,
    C=\bmatr{1}{2 \\ 1 \\ 3 \\ 0 \\ 1}
    .$$
\item These have types $A=A_{4\times 2}, B=B_{1\times 5},C=C_{5\times 1}$.

 \item The matrix $B$ above is said to be {\bf a row matrix} and $C$ is said
 to be {\bf a column matrix} for obvious reasons.
 
  \end{itemize}
\end{frame}

%2

\begin{frame}%3
  \frametitle{Matrix operations.}
  \begin{itemize}%[<+-| alert@+>]

\item Individual entries of a matrix are conveniently denoted by a
subscript notation. Thus for the matrix $A$ above, we have $A_{2 2}=4$
and we may find it more convenient to  write it as $A(2,2)=4$.

Note that
$$A(2,1)=A(4,1)=3 = B(1,3) = C(3,1).$$

\item {\bf Addition.} Let $P,Q$ be matrices of the same type. We define
$P+Q$ by the formula
$$(P+Q)(i,j) = P(i,j)+Q(i,j).$$

If the types are mismatched, then {\bf the sum is undefined.}

\item {\bf Scalar Multiplication.} Let $P$ be a matrix and $c$ any
number (which is also called a scalar. 
Then we define $cP$ by the formula
$$(cP)(i,j)=cP(i,j).$$


\end{itemize}
\end{frame}

%3

\begin{frame}%4
  \frametitle{More definitions.}
  \begin{itemize}%[<+-| alert@+>]
 
\item {\bf Example.}
Let
$$P=\bmatr{2}{2 & 5 \\ 1 & 7} \mbox{ and }
Q = \bmatr{2}{3 & 1 \\ -1 & 2}.$$
Let $R = P+Q$.
%\pause

Then
$$R = \bmatr{2}{2+3 & 5+1 \\ 1-1 & 7+2 } = \bmatr{2}{5 & 6\\0 & 9}.$$ 
 

\item 
Also, if $c=5$ then
$$cP = \bmatr{2}{5\cdot 2 & 5\cdot 5 \\ 5\cdot 1 & 5\cdot 7}
= \bmatr{2}{10 & 25 \\ 5 & 35}. $$

\item Thus:
$$2P-3Q = \bmatr{2}{2\cdot  2 - 3\cdot 3 & 2\cdot 5 - 3\cdot 1 \\
                    2\cdot 1 - 3\cdot (-1) & 2\cdot 7 - 3\cdot 2 }
        =\bmatr{2}{-5 & 7 \\ 5 & 8}.$$
        
\end{itemize}

\end{frame}

%4

\begin{frame}%5
  \frametitle{Matrix Product.}
  \begin{itemize}%[<+-| alert@+>]
 
\item Given matrices $A=A_{m\times n}$ and $B=B_{r \times s}$ we define
their product {\bf only when } $n=r$. This means that the number of
columns of $A$ matches the number of rows of $B$.

\item The definition is:
$$(AB)(i,j) = A(i,1)B(1,j)+A(i,2)B(2,j)+\cdots +A(i,n)C(n,j).$$

\item Thus for our earlier examples:
$$ B=\bmatr{5}{2 & -1 & 3 & 0 & 1}~,
    C=\bmatr{1}{2 \\ 1 \\ 3 \\ 0 \\ 1}$$
we define
$$BC = \bmatr{1}{2 \cdot 2+ (-1)\cdot (1) + 3 \cdot 3 + 0 \cdot 0 + 1 \cdot 1 }
= \bmatr{1}{13}.$$

\end{itemize}
\end{frame}

%5

\begin{frame}%6
  \frametitle{Understanding the Matrix Product.}
  \begin{itemize}%[<+-| alert@+>]
 
\item Thus, if we multiply a row and a column of the same length we get
a $1\times 1$ matrix and it is often written as a single number without
the square brackets.

\item This helps us understand the general product thus.

\item Consider the old 
$$P=\bmatr{2}{2 & 5 \\ 1 & 7} \mbox{ and }
Q = \bmatr{2}{3 & 1 \\ -1 & 2}.$$

\item Write $P = \bmatr{1}{R_1\\R_2}$ where $R_1,R_2$ are its two rows
$\bmatr{2}{2 & 5} $ and $\bmatr{2}{1 & 7}$.

Similarly, write $Q=\bmatr{2}{C_1 & C_2}$ where
$C_1,C_2$ are its columns $\bmatr{1}{3 \\ -1}, \bmatr{1}{1 \\ 2}$.

\end{itemize}
\end{frame}

%6

\begin{frame}%7
  \frametitle{Matrix product explained.}
  \begin{itemize}%[<+-| alert@+>]
 
\item Then we have:
$$PQ =\bmatr{1}{R_1\\R_2} \bmatr{2}{C_1 & C_2}
=\bmatr{2}{R_1C_1 & R_1C_2 \\ R_2 C_1 & R_2 C_2}.$$

\item Thus, we see
$$PQ 
= \bmatr{2}{2 \cdot 3 + 5\cdot (-1) & 2 \cdot 1 + 5 \cdot 2 \\
                 1 \cdot 3 + 7 \cdot (-1) & 1 \cdot 1 + 7 \cdot 2 }
     = \bmatr{2}{1 & 12 \\-4 & 15}.$$


\end{itemize}
\end{frame}

%7

\begin{frame}%8
  \frametitle{Another view of the product.}
  \begin{itemize}%[<+-| alert@+>]
 
\item Here is another useful view of a matrix product. Suppose that we
have a matrix $A=A_{m\times n}$ and we multiply it by a column
$X=X_{n\times 1}$. Then we can interpret $AX$ thus:

Write
$$A=\bmatr{4}{C_1 & C_2 & \cdots & C_n} \mbox{ and }
 X = \bmatr{1}{x_1\\ x_2\\ \cdots \\ x_n}.$$

Then $$AX=x_1 C_1 + x_2 C_2 + \cdots + x_n C_n.$$

\end{itemize}
\end{frame}

%8

\begin{frame}%9
  \frametitle{Calculating the full product.}
  \begin{itemize}%[<+-| alert@+>]

\item Thus we can find the same old $PQ$ by multiplying $P$ by each
column of $Q$ and building a matrix from them.

\item We already know: 
$$\bmatr{2}{2 & 5 \\ 1 & 7} \bmatr{1}{3 \\ -1} =
3\bmatr{1}{2 \\ 1} + (-1)\bmatr{1}{5 \\ 7} = \bmatr{1}{1\\-4}.$$

Similarly:

$$\bmatr{2}{2 & 1 \\ 5 & 7} \bmatr{1}{1 \\ 2} =
1\bmatr{1}{2 \\ 1} + 2\bmatr{1}{5 \\ 7} = \bmatr{1}{12\\15}.$$

This gives another way to get $PQ$ by as a matrix with these two
columns. Both these methods are useful.


\end{itemize}
\end{frame}

%9

\begin{frame}%10
  \frametitle{Special Matrices.}
  \begin{itemize}%[<+-| alert@+>]
\item A matrix is said to be {\bf  square}, if its type is $n\times n$
for some $n$.

\item Note that the product $AA$ is defined if and only if $A$ is
square. If $A$ is square, then we use the more natural notation $A^2$ in
place of $AA$.

More generally $A^m$ is defined as $AA\cdots A$ where we have exactly
$m$ terms of a square matrix $A$.

\item
A matrix with all zero entries is called a {\bf zero matrix} and we
abuse the notation by simply writing it as $0$. It may be written as
$0_{m\times n}$ to indicate its type, but this is rarely done.

Thus the size of this $0$  must be guessed  by context.

\item Another matrix which has an abused notation is the {\bf identity
or unit matrix.} It is denoted by $I$ or $I_n$ if its type is
$n\times n$ matrix.

The identity matrix is defined by the formula:
$$I(i,j) = 0 \mbox{ if } i\neq j \mbox{ and } I(i,i)=1.$$

\end{itemize}
\end{frame}
%10

\begin{frame}%11
  \frametitle{More Special Matrices.}
  \begin{itemize}%[<+-| alert@+>]
\item Thus, we have
$$I_2 = \bmatr{2}{1 & 0 \\ 0 & 1} \mbox{ and } I_3 =
\bmatr{3}{1 & 0 & 0 \\
          0 & 1 & 0 \\
          0 & 0 & 1 }.$$

\item These behave like $1$ in our usual numbers. Thus
$AI = A \mbox{ and } IA = A $
for all matrices $A$ with the understanding that the $I$ is chosen to
make the product well defined.




\end{itemize}
\end{frame}

%11
\begin{frame}%12
  \frametitle{ Elementary Matrices.}
  \begin{itemize}%[<+-| alert@+>]
 
\item We performed some elementary operations on matrices to achieve
desired forms like REF and RREF. We now show that these operations can
be also explained as simply the result of multiplying by suitable
special matrices called elementary matrices. We first define them.


\item Let $n>1$ be a chosen integer.
Define $E_{rs}(c)$ to be the $n\times n $ matrix defined as follows.

Start with $I_n$ and write $c$ for its $(r,s)$-th entry.

Thus if $n=3$, then

$$E_{21}(5) = \bmatr{3}{1 & 0 & 0 \\
                        5 & 1 & 0 \\
                        0 & 0 & 1}
\mbox{ and }
E_{22}(5) = \bmatr{3}{1 & 0 & 0 \\
                      0 & 5 & 0 \\
                      0 & 0 & 1}.$$

\end{itemize}
\end{frame}
%12

\begin{frame}%13
  \frametitle{Using Elementary Matrices.}
  \begin{itemize}%[<+-| alert@+>]
\item Let
$$A = \bmatr{3}{1 & 1 & 2\\
                -5 & -4 & 0 \\
                0 & 1 & 3}.$$
\item Calculate $E_{21}(5)A$.

$$\bmatr{3}{1 & 0 & 0 \\
            5 & 1 & 0 \\
            0 & 0 & 1}
\bmatr{3}{1 & 1 & 2\\
                -5 & -4 & 0 \\
                0 & 1 & 3} =
\bmatr{3}{1 & 1 & 2\\
          0 & 1 & 10 \\
          0 & 1 & 3}.$$
\item Thus we note that {\bf multiplying on the left by }$E_{21}(5)$
has the same effect as our operation $R_2+5R_1$.

\item It can be proved that when $i\neq j$ then left multiplying by
$E_{ij}(c)$ has the same effect as $R_i+cR_j$.


\end{itemize}
\end{frame}
%14


\begin{frame}%15
  \frametitle{Using Elementary Matrices continued.}
  \begin{itemize}%[<+-| alert@+>]
 
\item What about $E_{ii}(c)$?

Consider  $E_{22}(5)A$.

\item
$$\bmatr{3}{1 & 0 & 0 \\
           0 & 5 & 0 \\
            0 & 0 & 1}
\bmatr{3}{1 & 1 & 2\\
                -5 & -4 & 0 \\
                0 & 1 & 3} =
\bmatr{3}{1 & 1 & 2\\
          -25 & -20 & 0 \\
          0 & 1 & 3}.$$
\item Thus $E_{22}(5)$ has the same effect as $5R_2$ when
we left multiply by it.

\item In general $E_{ii}(c)$ has the same effect of $cR_i$.
\end{itemize}
\end{frame}
%15
%
\begin{frame}%16
  \frametitle{The Voodoo Principle.}
  \begin{itemize}%[<+-| alert@+>]
 
\item How should we remember the effect of elementary matrices?
Here is a simple trick.

\item Say we wish to do the operation $R_2+5R_1$ on some $3\times
n$ matrix.

Since it has $3$ rows, we start with $I_3$ and perform the desired
operation on it.

$$\bmatr{3}{1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1}
\stackrel{R_2+5R_1}{\longrightarrow}
\bmatr{3}{1 & 0 & 0 \\ 5 & 1 & 0 \\ 0 & 0 & 1}.$$

Thus we get $E_{21}(5)$ by doing the operation on $I$. We can now left
multiply by it to any matrix to get the same row operation.


\end{itemize}
\end{frame}
%16

\begin{frame}%17
  \frametitle{Voodoo Principle continued.}
  \begin{itemize}%[<+-| alert@+>]
 
\item Thus we see:

$$\bmatr{3}{1 & 0 & 0 \\ 5 & 1 & 0 \\ 0 & 0 & 1}
\bmatr{4}{1 & 2 & 0 & -1\\
          -5 & 1 & 1 & 2\\
          1 & 1 & 0 & 1}
\longrightarrow
\bmatr{4}{1 & 2 & 0 & -1\\
          0 & 11 & 1 & -3\\
          1 & 1 & 0 & 1}
          .$$
\item Any desired row operation can be thus performed. For instance if
we want the sum of the three rows of the above matrix, we can simply
multiply it by the sum of the three rows of $I_3$, i.e. $\bmatr{3}{1 & 1
& 1}$.
Try this out!

\item We can perform column operations as well, except we need to
multiply by the modified identity matrices on the {\bf right} instead of
left.

\end{itemize}
\end{frame}
%17

\begin{frame}%18
  \frametitle{Permutation matrices.}
  \begin{itemize}%[<+-| alert@+>]
 
\item The Voodoo principle can also be used to permute rows of a given
matrix. Thus, for any matrix $A=A_{3\times n}$  we wish to swap its
second and third rows.

\item We start with $I_3$ (an identity  matrix with the same number of
rows and swap its second and third rows.
We get:
$$P_{23} = \bmatr{3}{1 & 0 & 0\\
                    0 & 0 & 1\\
                    0 & 1 & 0}.$$
\item Now $P_{23}A$ gives:
$$\bmatr{3}{1 & 0 & 0 \\
          0 & 0 & 1\\
          0 & 1 & 0}
\bmatr{3}{1 & 1 & 2\\
          -5 & -4 & 0 \\
           0 & 1 & 3} =
\bmatr{3}{1 & 1 & 2\\
          0 & 1 & 3 \\
          -5 & -4 & 0}.$$
\end{itemize}
\end{frame}
%18
\begin{frame}%19
  \frametitle{More Voodoo Principle.}
  \begin{itemize}%[<+-| alert@+>]
 
\item In general a permutation matrix is a matrix obtained by permuting
rows of some $I_n$. Left multiplication by such a permutation matrix
produces the
same row permutation on any chosen matrix (with $n$ rows).

\item The same permutation matrix can also be interpreted as a column
permutation. Thus our $P_{23}$ above can be thought of as swapping the
second and third columns of $I_3$.

\item Then $AP_{23}$ will do the same column permutation of $A$ thus:
$$\bmatr{3}{1 & 1 & 2\\
          -5 & -4 & 0 \\
           0 & 1 & 3}
\bmatr{3}{1 & 0 & 0 \\
          0 & 0 & 1\\
          0 & 1 & 0}
 =
\bmatr{3}{1 & 2 & 1\\
          -5 & 0 & -4 \\
          0 & 3 & 1}.$$

\end{itemize}
\end{frame}

\begin{frame}%2 

  \frametitle{Matrices for solving equations.}
  Now we describe the main use of matrices for solving systems of linear
  equations. In this lecture, we would mainly consider systems where the
  number of equations equals the number of variables.
 
  \begin{itemize}%[<+-| alert@+>]

    \item A linear system of $n$ equations in $n$ variables can be
    descibes by a single matrix equation of the form $AX=B$.



\end{itemize}

\end{frame}

%2
\begin{frame}%3
  \frametitle{Examples.}
  \begin{itemize}%[<+-| alert@+>]
\item
    For example:
   
The equations $2x-3y=1,x-2y=5$ can be written as
$$\bmatr{2}{2 & -3\\ 1 & -2} \bmatr{1}{x\\y} ~=~ \bmatr{1}{1 \\ 5}.$$
  

\item The solution, in turn can also be described as $IX=C$ which
reduces to $X=C$.

\item
Thus, the solution to the above system is $x=-13, y=-9$ and can be
written as:
$$\bmatr{2}{1 & 0\\ 0 & 1}\bmatr{1}{x\\y} ~=~\bmatr{1}{-13\\-9}.$$



\end{itemize}
\end{frame}

%3

\begin{frame}%4
  \frametitle{Examples continued.}
  \begin{itemize}%[<+-| alert@+>]
  
\item %x-y-z,x+y+2\,z,2\,x+y+3\,z%
The equations $x-y-z=5,x+y+2z =0 ,2x+y+3z =1$ can be written as
$$\bmatr{3}{ 1 & -1 & -1 \\ 1 & 1 & 2\\2 & 1 & 3}\bmatr{1}{x\\y\\z}
~=~ \bmatr{1}{5\\0\\1}.$$
\item The solution, as before can be written as $IX=C$ which
reduces to $X=C$.
Thus, the solution to the above system is $x=4, y=2, z=-3$ and can be
written as

$$\bmatr{3}{1 & 0 &0 \\ 0 & 1 & 0 \\ 0 & 0 & 1}\bmatr{1}{x\\y\\z}
 ~=~\bmatr{1}{4\\2\\-3}.$$
 





\end{itemize}
\end{frame}

%4


\begin{frame}%5
  \frametitle{Inverse Philosophy.}
  \begin{itemize}%[<+-| alert@+>]
\item Both the solutions above can be described by the following simple
philosophy. 

Let the original equations be $AX=B$ where we assume that $A$ is a
square $n\times n$ matrix, $X$ is the column of $n$ variables and $B$
denotes the right hand sides.

We {\bf find} an $n\times n$ matrix $M$ such that $AM=MA=I_n$.
\item 
 Multiplying both sides of the equation $AX=B$ by $M$ on the left, we get
$MAX=MB$  which becomes
$$IX=MB \mbox{ and yields the solution } X=MB.$$


\item Thus, it would be good to have a mechanism for finding  such a
matrix $M$ when possible.


\end{itemize}

\end{frame}  
%4

\begin{frame}%5
  \frametitle{Inverse Defined.}
  \begin{itemize}%[<+-| alert@+>]
 


\item We define the inverse of a {\bf square matrix} $A$ to be a square
matrix $M$ such that $MA=AM=I$.

\item The matrix $M$ can be shown to be uniquely defined by $A$, when it
exists and is called the {\bf inverse of $\mathbf{A}$.} 

\item The matrix $A$ is said to be invertible (or non singular) if its
inverse exists and it is said to be non invertible or singular
otherwise.

\end{itemize}
\end{frame}

%5

\begin{frame}%6
  \frametitle{Finding the Inverse $\mathbf{2\times 2}$ case.}
  \begin{itemize}%[<+-| alert@+>]
 
\item If $$A=\bmatr{2}{a & b\\c& d}$$ then we have a very simple answer.

Let $\Delta=\det(A)=ad-bc$.
\item Then $A$ is invertible if and only if $\Delta\ne 0$.
\item Moreover, if $\Delta\ne 0$ then the inverse of $A$ is the matrix
$$\frac{1}{\Delta}\bmatr{2}{d & -b \\ -c & a}.$$


\item Thus, for our first example above $A=\bmatr{2}{2 & -3 \\ 1 & -2}$.
Then $\Delta=2 \cdot (-2) - 1\cdot (-3) = -1$ and hence the inverse is
$$M=\frac{1}{-1}\bmatr{2}{-2 & 3 \\ -1 & 2} = \bmatr{2}{2 & -3 \\ 1 &
-2}.$$

\end{itemize}
\end{frame}

%6

\begin{frame}%7
  \frametitle{Inverse Calculation continued.}
  \begin{itemize}%[<+-| alert@+>]
 

\item It is easy to check that $M\bmatr{1}{1\\5} = \bmatr{1}{-13\\-9}$
is the old solution.
%\pause
As a side note, we observe that in this case the inverse $M$ is the same
as $A$, or $AA=A^2=I$. Such matrices are said to be {\bf unipotent}.

\item {\bf Important notation.} When the inverse of $A$ exists, it is
denoted by the convenient notation $A^{-1}$.

{\bf Do not ever } write $\frac{1}{A}$ in place of $A^{-1}$; it is both
illegal and meaningless.

\end{itemize}
\end{frame}

%7

\begin{frame}%8
 \frametitle{An observation.}
  \begin{itemize}%[<+-| alert@+>]

\item We solved the equation $AX=B$ above as $X=MB=A^{-1}B$ for a
specific $2\times 2$ matrix $A$. Note that $B$ did not enter the
calculation until the product $MB$.

\item
Thus we observe that if  $A$ is invertible, then the equation $AX=B$ has
a unique solution $X=A^{-1}B$.

\item This should be compared with the statement:
If a,b are numbers and  if $a\ne 0$ then the equation
$ax=b$ has a uniques solution $x=\frac{b}{a}$.

\item What happens if $\Delta=0$. Let $P=\bmatr{2}{1 & 2 \\ 2 & 4}$ and
consider the equations $PX=Q$ where $Q=\bmatr{1}{u\\v}$.

\item We invite you to check that when $v=2u$ this system has
infinitely many solution, but it has no solution when $v\ne 2u$.


\end{itemize}
 
\end{frame}

%8

\begin{frame}%9
 \frametitle{General Inverses.}
  \begin{itemize}%[<+-| alert@+>]
 
\item  Now we discuss the general inverse. the formula is not as
convenient as in the $2\times 2$ case. So we give a procedure.

\item Suppose we are trying to find the inverse of a matrix
$A=A_{n\times n}$.

Start with the augmented matrix $\left[A|I\right]$ and row reduce it, i.e. find its
RREF.

\item The matrix $A$ is invertible if and only if the RREF becomes $\left[I|M\right]$
for some $n\times n$ matrix $M$. Moreover, this $M$ is the desired
$A^{-1}$.

\item If one of the pivots is on the right hand side of the separator
bar, then the matrix $A$ is non invertible or singular.
\end{itemize}
 
\end{frame}

%9

\begin{frame}%10
  \frametitle{Example of inverse.}
  \begin{itemize}%[<+-| alert@+>]
\item
We now illustrate the procedure on our $3\times 3$ matrix in the second
example.

\item Start with:
$$\left[A|I\right] =
\left[ \begin {array}{ccc|ccc} 1&-1&-1&1&0&0\\\noalign{\medskip}
1&1&2&0&1&0\\\noalign{\medskip}
2&1&3&0&0&1\end {array} \right]
.$$

\item When we do row transformations $R_2-R_1, R_3-2R_1$ and
$R_3-\frac{3}{2}R_2$, we get
$$\left[ \begin {array}{ccc|ccc}
1&-1&-1&1&0&0\\\noalign{\medskip}
0&2&3&-1&1&0\\\noalign{\medskip}
0&0&1/2&-1/2&-3/2&1\end {array}
\right].$$
This is REF.
\end{itemize}
\end{frame}
%10

\begin{frame}%11
  \frametitle{Inverse continued.}
  \begin{itemize}%[<+-| alert@+>]
\item Now we go on to make RREF.

\item The operations
$$2R_3, R_2-3R_3, R_1+R_3, \frac{1}{2}R_2, R_1+R_2$$
produce the RREF:
$$\left[ \begin {array}{ccc|ccc} 1&0&0&1&2&-1\\\noalign{\medskip}
0&1&0&1&5&-3\\\noalign{\medskip}
0&0&1&-1&-3&2\end {array} \right]
$$

\item Thus the desired inverse is:
$$
A^{-1}=\left[ \begin {array}{ccc} 1&2&-1\\\noalign{\medskip}
1&5&-3\\\noalign{\medskip}
-1&-3&2\end {array} \right]
$$.

\end{itemize}
\end{frame}

%11
\begin{frame}%12
  \frametitle{ Summary.}
  \begin{itemize}%[<+-| alert@+>]

\item Verify that our answer is correct.

Thus: 
$$\left[ \begin {array}{ccc} 1&2&-1\\\noalign{\medskip}
1&5&-3\\\noalign{\medskip}
-1&-3&2\end {array} \right] \bmatr{1}{5\\0\\1}
~=~ \bmatr{1}{4 \\ 2\\-3}.$$

\item We note that when we finish the work of converting
$\left[A|I\right]$ we can either find the inverse or determine that the
inverse does not exist.

\item If $A$ has an inverse, then the equations $AX=B$ always have a
uniques solution $X=A^{-1}B$.


\item The main drawback of this method is the calculation of the RREF
which can be lengthy.

\end{itemize}
\end{frame}
%12

\begin{frame}%13
  \frametitle{Testing Invertibility.}
  \begin{itemize}%[<+-| alert@+>]
\item Thus, it would be useful to know if we are likely to find an
inverse before doing the full work.

\item Luckily, we already have such a tool. Let $A$ be an $n\times n$
matrix.

We can convert
$\left[A|I\right]$ to REF. It is easy to see that we have exactly $n$
pivots.

\item The inverse exists if and only if all the pivots are on the left
hand side of the separator bar.

It is instructive to observe the following:
$$\left[
\begin{array}{cc|cc}
1 & 2 & 1 & 0\\
2 & 4 & 0 & 1
\end{array}
\right] ~~ R_2-2T_1\Rightarrow
\left[
\begin{array}{cc|cc}
1 & 2 & 1 & 0\\
0 & 0 & -2 & 1
\end{array}
\right]
$$

Since the second row has pivot on RHS, we have no inverse!
\end{itemize}
\end{frame}
