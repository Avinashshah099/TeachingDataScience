%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Notes from Oxford}
https://courses.maths.ox.ac.uk/node/download\_source/14685
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{ Differentiability of a function of one variable:}

Let $I \subseteq \mathbb{R}$ be an open interval. A function $f\colon I \subseteq \mathbb{R} \rightarrow \mathbb{R}$
is differentiable in $x \in I$ if 

  \[ f'(x):= \lim_{h \rightarrow 0} {f(x+h) - f(x) \over h}  \]
 
exists.  

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{ Differentiability of a function of one variable:}
  
  Equivalently we can say that $f$ is differentiable in  $x\in I$ if there
exists a linear map   $L\colon \mathbb{R} \rightarrow \mathbb{R}$  such that

  \begin{equation}\label{diff1}
  \lim_{h \rightarrow 0} {f(x+h) - f(x) -Lh \over h} = 0.
  \end{equation}
  
  In this case we write $f'(x)=L$.

  Another way of writing \eqref{diff1} is  
  \begin{equation} \label{diff2}
  f(x+h)-f(x) - Lh = R_f(h) \text{ with } R_f(h) = o(|h|), \; \text{ i.e. } 
  \; \lim_{h \to 0} \frac{R_f(h)}{|h|} =0.
  \end{equation}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{ Differentiability of a vector-valued function of one variable:}

Completely analogously we define the derivative of a vector-valued function of 
one variable. More precisely,  if 
  $f\colon I \subseteq \mathbb{R} \rightarrow \mathbb{R}^m, m>1$, with components
  $f_1, \ldots, f_m$, we say that $f$ is differentiable in $x \in I$ if 
  \[ \ 
  f'(x):=\lim_{h \rightarrow 0} {f(x+h) - f(x) \over h} \]
  exists.  

 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{ Differentiability of a vector-valued function of one variable:}

   It is easily seen that $f$ is differentiable in $x \in I$ if and only if 
  $f_i \colon I \subseteq \mathbb{R} \rightarrow \mathbb{R}$
  is differentiable in  $x \in I$ for all  $i = 1, \ldots, m$. Also, 
  $f$ is differentiable in  $x \in I$  if and only if there exists a linear map
  $L\colon \mathbb{R} \rightarrow \mathbb{R}^m$ 
  such that \[\lim_{h \rightarrow 0} {f(x+h) - f(x) -Lh \over h} = 0.\]
  Here and in what follows we write $Lh$ instead of $L(h)$ if $L$ is a linear map.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}
 
 \begin{itemize}
\item How can we now generalize the concept of differentiability to functions of several variables,
\item Say for a function $f\colon \Omega \subseteq \mathbb{R}^2 \to \mathbb{R}$, $f=f(x,y)$?
\item A natural idea is to freeze one variable, say $y$, define $g(x):=f(x,y)$ and check whether
$g$ is differentiable in $x$. 
\item  This will lead to the notion of partial derivatives and most
of you have seen this already in lectures in the first year, e.g. in Calculus.
\item  However, we will see that the concept of partial derivatives alone is not completely
satisfactory.
\item For example we will see that the existence of partial derivatives does not
guarantee that the function itself is continuous (as it is the case for a function of one variable).

\end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}
 \begin{itemize}
\item  The notion of the (total) derivative for functions of several
variables will not have this deficiency. 
\item  It is based on 
a generalisation of the formulation in \eqref{diff1}.
In order to do that we will need a suitable norm in $\mathbb{R}^n$. 
\item  All
norms in $\mathbb{R}^n$ are equivalent, and hence 
properties of sets, such as openness or boundedness, and of functions,
such as continuity, do not depend on the choice of the norm.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}
In the sequel we will always use the {\it Euclidean norm } in 
$\mathbb{R}^n$ and denote it by $|\cdot |$. More precisely,
 for $x=(x_1, \dots,x_n) \in \mathbb{R}^n$ we denote
\[
|x|:= \sqrt{x_1^2 + \dots x_n^2}\,.
\]
You may check yourself that this defines a norm. For the proof
of the triangle inequality you use the Cauchy-Schwarz inequality. 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Partial derivatives}
 \begin{itemize}
\item We are going to consider functions
$f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m$.
\item Here and in what follows we always assume that 
$\Omega$ is an open and connected  subset of $\mathbb{R}^n$ (a domain).

\item We first consider a few examples of such functions \ldots
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Partial derivatives}




  \begin{itemize}
  \item[a)] Let $f\colon \Omega \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$.
  be a function. then the graph of $f$, given by
    \[\Gamma_f = \{ (x,y) \in \Omega \times \mathbb{R} \mid y = f(x) \}.\]
    is usually a surface in   $\mathbb{R}^3$. Its level set at level $c\in \mathbb{R}$  is
    \[N_f(c) = \{ x \in \Omega \mid f(x) = c \},\] 
    which is usually a curve in $\Omega$.
  \item[b)] The electric field in a vacuum induced by a point charge $q$ in a point
   $x_0 \in \mathbb{R}^3$  is given by
   \[f\colon \mathbb{R}^3 \setminus \{ x_0 \} \rightarrow \mathbb{R}^3\,,
    \qquad f(x) = q {x - x_0 \over |x - x_0|^3}.\]
  \item[c)] $f\colon \mathbb{C} \rightarrow \mathbb{C}, f(z) = e^z$
    has -- with the usual identifications -- 
    the real representation: $f\colon \mathbb{R}^2 \rightarrow \mathbb{R}^2$
    \[f(x,y) = \binom{e^x \cos y}{e^x \sin y}.\]
  \end{itemize}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Partial derivatives}
 
\begin{block}{Partial derivative}

  Let  $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}$.
  We say that the i-th partial derivative of $f$ in  $x \in \Omega$ exists, if
  \[ \frac{\partial f}{ \partial x_i}(x) := \lim_{t \rightarrow 0} {f(x+t e_i) - f(x) \over t}\]
  exists, where 
   $e_i$ is the  $i$-th unit vector.
   In other words, the i-th partial derivative is the derivative of $g(t):= f(x+te_i)$ in $t=0$.

  Other common notations for the i-th partial derivative of $f$ in $x$ are
  \[
  \partial_i f(x), \quad D_if(x),  
  \quad \partial_{x_i} f(x),
  \quad  f_{x_i}(x) \quad \text{ or } \quad f_{,i}(x).
  \]
  We will mostly use $\partial_i f(x)$.
  If $f\colon \Omega \subseteq \mathbb{R}^2 \to \mathbb{R}$ we often write $f=f(x,y)$
  and denote the partial derivatives by $\partial_x f$ and $\partial_y f$ respectively.
  \end{block}
  
  \end{frame}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Examples}

  \begin{itemize}
  \item[a)] 
    Let $f\colon \mathbb{R}^n  \rightarrow \mathbb{R} $ be given by $
    f(x) = |x|$. Then for $x \not=0$ we have
    \[
    \frac{1}{t} \big( |x+ t e_i| - |x| \big) = \frac{1}{t} \frac{2 t x_i + t^2}{|x+te_i| + |x| } =
    \frac{ 2 x_i + t}{|x+te_i| + |x|} \to \frac{x_i}{|x|} 
    \]
    as $t \to 0$. Hence, for $x\not=0$, the function $f$ has partial derivatives, given by
    $\partial_i f(x) = \frac{x_i}{|x|}$.
    Notice that no partial derivative of $f$ exists in $x=0$.
  \item[b)] Let $f(x) = g(r)$  with
    $r(x) = |x|$  and differentiable $g\colon [0,\infty) \rightarrow \mathbb{R}$. Then, for $x \not=0$, by the Chain 
   Rule from Mods Analysis 2,
    we find
    \[\partial_i f(x)
    = g'(r)  \partial_i r(x) 
    = g'(r) {x_i \over |x|} 
    = {g'(r) \over r}  x_i\,.\]
  \end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{ Examples}
The following example shows that, surprisingly,  functions whose partial derivatives all exist  are in
general not continuous.

Let
  $f\colon \mathbb{R}^2 \rightarrow \mathbb{R}$ 
  \[
  \Fall{f(x,y) }{x y \over (x^2+y^2)^2}{(x,y) \not=(0,0)}{0}{(x,y)=(0,0)}
  \]
  \begin{itemize}
  \item $f$ has partial derivatives on $\mathbb{R}^n \setminus \{0\}$
  with
    \[\partial_x f(x,y) 
    = {y
      \over (x^2+y^2)^2} 
      - \frac{4 x^2y}{(x^2+y^2)^3}\]
    and a similar expression for $\partial_y f(x,y)$.

    \item
    {\bf But:} $f$ is not continuous in  $(x,y)=(0,0)$: to see this, consider the sequence
     $(x_k,y_k) = ({1 \over k},  {1 \over k})$ for which
     $x_k^2+y_k^2 = {2 \over k^2} \rightarrow 0$ 
    as $k \rightarrow \infty$.
   Then $f(x_k,y_k) = \frac{k^4}{ 4k^2} \to \infty$ 
    as $k \rightarrow \infty$.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{ Examples}

  We will see later that the generalized version of differentiability will imply continuity
  of the function. In particular, functions with continuous partial derivatives are also
  continuous. (In our example the partial derivatives $\partial_if(x)$ are not continuous in $x=0$.)


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Directional derivative}

The partial derivative is a special case of a directional derivative which we will define now.



 Suppose that  
  $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}$ and  $v \in \mathbb{R}^n \backslash {0}$.
  If 
  \[ \partial_v f(x) :=
   \lim_{t \rightarrow 0} {f(x+tv) - f(x) \over t} \]
  exists, we call it the 
   \emph{directional derivative } of  $f$ in direction $v$ at the point $x \in \Omega$.


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Example}

  Let $f\colon \mathbb{R}^n \rightarrow \mathbb{R}$ be given by  $f(x) = |x|$ 
   and let $x , v \in \mathbb{R}^n \setminus \{0\}$.
  Then
  \begin{eqnarray*}
    \partial_v f(x)
    & = & {\dop\over \dt}\Big |_{t=0} |x+tv| 
     =  {\dop\over \dt} \Big |_{t=0} \Big(\sum_{i=1}^n |x_i + t v_i|^2\Big)^{1 \over 2} \\
    & = & {1 \over 2|x|} \sum_{i=1}^n 2 x_i v_i
     =  \sum_{i=1}^n {x_i \over |x|} v_i
     = \big \langle {x \over |x|}, v \big \rangle\,,
  \end{eqnarray*}
  where $\langle \cdot, \cdot \rangle$
  denotes the usual scalar product in $\mathbb{R}^n$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Gradient}


  Let $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}$ 
  and assume that all partial derivatives exist in $x \in \Omega$.
  We call the vector field $\nabla f(x)\in \mathbb R^n$ given by
  \[\nabla f(x) 
  := \left( \begin{array}{c}
      \partial_1 f(x)\\
      \vdots\\
      \partial_n f(x)
    \end{array} \right)\]
  the \emph{gradient} of $f$ in $x$.


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Example}

  If $f(x) = |x|$
    and $x \neq 0$ then $\nabla f(x) = 
    \left( \begin{array}{c}
        {x_1 \over |x|}\\
        \vdots\\
        {x_n \over |x|}
      \end{array} \right) 
    = {x \over |x|}$\\
    and $\partial_v f(x) = \left< \nabla f(x), v \right>$


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Partial derivatives for vector-valued functions}

  Let $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m$.
  We say that  the i-th partial derivative of $f$ in $x \in \Omega$ exists, if it
  exists for 
   all components $f_1, \ldots, f_m$. 
  In that case we write
  $\partial_i f(x) = 
  \left( \begin{array}{c}
      \partial_i f_1(x)\\
      \vdots\\
      \partial_i f_m(x)
    \end{array} \right)$.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Jacobian matrix}


  Suppose that $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m$ 
 and that all partial derivatives exit in $x \in \Omega$. Then the $(m\times n)$-matrix
  \[ D f(x) := \left( \begin{array}{c c c}
      \partial_1 f_1(x) & \ldots & \partial_n f_1(x)\\
      \vdots & \ddots & \vdots\\
      \partial_1 f_m(x) & \ldots & \partial_n f_m(x)
    \end{array} \right) \]
  is called the \emph{Jacobian matrix} of $f$ in the point $x$.\\
  If  $n=m$ we call $J_f(x) := \det D f(x)$
  the \emph{Jacobian determinant} or the \emph{functional determinant} of $f$ in $x$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Jacobian matrix}
We can write the Jacobian matrix in terms of the gradients of the components:
  \[ D f(x) 
  = \left( \begin{array}{c} 
      (\nabla f_1(x))^T\\
      \vdots\\ 
      (\nabla f_m(x))^T
    \end{array} \right)\,, \]
    where the superscript $T$ denotes transposition.
    
    \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Higher partial derivatives}
Partial derivatives of order  $k$, 
  $C^k(\Omega, \mathbb{R}^n)$

\begin{itemize}
\item[a)]
  Let $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m$. 
  The \emph{partial derivatives} of order $k$ 
  are inductively defined via
  \[ \partial_{j_1} \ldots \partial_{j_k} f(x) 
  := \partial_{j_1} (\partial_{j_2} \ldots \partial_{j_k}) f(x) 
  \quad \text{where} \quad j_1, \ldots, j_k \in \{ 1, \ldots, n \}\]
  Notice that $j_1, \ldots, j_k$ are not necessarily distinct.
  A common notation is
   $\frac{ \partial^k f(x)}{ \partial_{j_1}\ldots \partial_{j_k}} $ or 
   -- very brief -- $\partial_{j_1 j_2 \ldots j_k} f(x)$. 

\item[b)]
  $C^k(\Omega, \mathbb{R}^m)$ is the set of continuous functions whose
  partial derivatives exist up to order $k$ for all $x \in \Omega$
  and are continuous in $\Omega$.
  If $m=1$ we write $C^k(\Omega)$.
  \end{itemize}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Exchangeability of partial derivatives (Theorem of Schwarz)}

  Suppose that  $f \in C^2(\Omega)$. Then we have for  $1 \le i,j \le n$ and for any $x \in \Omega$
  that
  \[\partial_i \partial_j f(x) = \partial_j \partial_i f(x)\]

\begin{proof} 
Let
   $\partial_j^t f(x) = {f(x+t e_j) - f(x) \over t}$
be the difference quotient of 
  $f$ in $x_j$.
  By definition
  \[\partial_i \partial_j f(x) 
  = \lim_{s \rightarrow 0} (\lim_{t \rightarrow 0} \partial_i^s \partial_i^t f(x))\]
  
  We need to show that both limits can be interchanged.
  By the Intermediate Value Theorem we have for all functions
   $g\colon \Omega \rightarrow \mathbb{R}$, 
  for which $\partial_i g(x)$ exists, that
  \[\partial_i^s g(x) = \partial_i g(x+\alpha s e_i) 
  \quad \text{ for some } \alpha \in (0,1).\]
  
  
\end{proof}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Exchangeability of partial derivatives (Theorem of Schwarz)}
\begin{proof}

  If we apply this for
   $g = \partial_j^t f$ and $g=\partial_i^s f$ it follows:
  \begin{eqnarray*}
    \partial_i^s \partial_j^t f(x) 
    & = & \partial_i \partial_j^t f(x+\alpha s e_i) 
    \quad \text{ for some } \alpha \in (0,1)\\
    & = & \partial_j^t (\partial_i f(x+\alpha s e_i))\\
    & = & \partial_j (\partial_i f(x+\alpha s e_i+\beta t e_j)) 
    \quad \text{ for some } \beta \in (0,1)
  \end{eqnarray*}
  Since
  $\partial_j \partial_i f$ is continuous it follows
  \[ \partial_j \partial_i f(x + \alpha s e_i + \beta t e_j)
  \xrightarrow{s \rightarrow 0} \partial_j \partial_i f(x + \beta t e_j)
  \xrightarrow{t \rightarrow 0} \partial_j \partial_i f(x).\]
\end{proof}

\begin{corollary}
Suppose that  $f \in C^k(\Omega)$. Then all partial derivatives up to order $k$ can
be interchanged.
\end{corollary}

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}
 
The following example shows that the condition in Proposition  that the second partial derivatives must be continuous is indeed necessary.

Let $f \colon \mathbb{R}^2 \to \mathbb{R}$ be given by
\[
\Fall{f(x,y)}{x y \frac{x^2 -y^2}{x^2 + y^2} }{(x,y) \not=(0,0)}{0}{(x,y)=(0,0)}\,.
\]
One can show that $f \in C^1 (\mathbb{R}^2)$, but $\partial_x \partial_y f(0,0)=1$ and $\partial_y \partial_x f(0,0)=-1$.


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Differentiability}


We will now introduce the notion of  the (total) derivative which is based on the idea
that the function can be approximated in a point by a linear map.


\begin{definition}(Differentiable map)

\begin{itemize}
\item[a)]
We say that a function
   $f\colon \Omega \subseteq \mathbb R^n \rightarrow \mathbb{R}^m$, 
   is \emph{differentiable} in 
   $x \in \Omega$ if there exists a linear map
  $L\colon \mathbb{R}^n \rightarrow \mathbb{R}^m$ such that
  \[ \lim_{h \rightarrow 0} {f(x+h) - f(x) - Lh \over |h|} = 0 \]
  We call $L$ the {\it (total) derivative }
  of $f$ in $x$ and denote it by $df(x)$.

\item[b)]
  We say that $f$ is differentiable in $\Omega$ if $f$ is differentiable in every
  $x \in \Omega$.
  \end{itemize}
\end{definition}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Differentiability}

  \begin{itemize}
  \item[a)] 
  Alternatively we can say that
    $f$ is in $x \in \Omega$ differentiable if there exists a linear map
     $L\colon \mathbb{R}^n \rightarrow \mathbb{R}^m$ 
     such that
    \[f(x+h) - f(x) - Lh
    = R_f(h) 
    \qquad \text{ with }  \quad
    R_f(h) = o(|h|)\]
  \item[b)] $f\colon \Omega \rightarrow \mathbb{R}^m$ is 
     differentiable in  $x \in \Omega$ 
     if and only if
     every component
    $f_i\colon \Omega \rightarrow \mathbb{R}, i=1, \ldots, m$,
    is differentiable in  $x \in \Omega$.
  \end{itemize}


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}
 Let $C = (c_{ij}) \in M_{n\times n}(\mathbb{R})$ be symmetric and let
    $f\colon \mathbb{R}^n \rightarrow \mathbb{R}$ be the quadratic form 
    corresponding to $C$, that is $f(x) = \left<x, Cx \right>$.
    Let
    $h \in \mathbb{R}^n$.
    \begin{eqnarray*}
      f(x+h) - f(x) &=& \left< x+h, C(x+h) \right> - \left<x, Cx \right>\\
    & = & \left<x, Cx\right> + \left<h, Cx \right> + \left<x, Ch \right>
    + \left< h, Ch \right> - \left<x, Cx \right>\\
    & \underset{C \text{ sym.}}{=} & 2 \left<Cx, h \right> + \left<h, Ch \right>
  \end{eqnarray*}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}

  Hence a candidate for $df(x)$ is 
   $(2 Cx)^T$. Indeed,
  \begin{eqnarray*}
    \left| {f(x+h) - f(x) - 2(Cx)^T h \over |h| } \right|
    & = & \left| { \left<h, Ch \right> \over |h| } \right|
    \leq  {|h| |Ch| \over |h|}\\
    &\le&
    |C| \, |h| 
    \rightarrow 0 \text{ for } h \rightarrow 0\,,
  \end{eqnarray*}
  where $ |C|= \left(\sum_{i,j=1}^n c_{ij}^2\right)^{1 \over 2}$.
  The last inequality follows, using the Cauchy-Schwarz inequality, via
  \[|Ch| = \Big(\sum_i \Big(\sum_j  c_{ij} h_j\Big)^2\Big)^{1 \over 2}
      \leq
      \Big(\sum_i \Big(\sum_j  c_{ij}^2\Big)
        \Big(\sum_j h_j^2\Big)\Big)^{1 \over 2}
      = |h| \, |C|.\]


Thus $f$ is differentiable in every 
  $x \in \mathbb{R}^n$ 
    and $d f(x) = (2Cx)^T$, that is $df(x)h = (2Cx)^Th= \langle 2Cx,h\rangle$.
    
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}



 $f\colon M_{n\times n}(\mathbb{R}) \rightarrow M_{n \times n}(\mathbb{R}), f(A) = A^2$.\\
    Let $H \in M_{n\times n}(\mathbb{R})$. Then
    \begin{eqnarray*}
      f(A + H) - f(A) & = & (A+H)(A+H) - A^2\\
      & = & AH + HA + H^2
    \end{eqnarray*}
    The linear term
    $ AH +HA$ is a candidate for the derivative:
      \[{f(A+H) - f(A) - (AH + HA) \over |H|}
       =  {H^2 \over |H|}
       \rightarrow  0 \qquad \text{ as } H \to 0.
    \]Hence $f$ is differentiable in every $A \in  M_{n \times n}(\mathbb{R})$ with $df(A)H = AH + HA$.


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}
  We are now going to establish the connection of the differential $df(x)$ with the 
  Jacobian matrix $Df(x)$.


If $f\colon\Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m$
is differentiable in $x \in \Omega$, then $f$ is continuous, the partial derivatives
$\partial_1 f(x), \dots, \partial_n f(x)$ exist and
\[
df(x)h = Df(x)h
\]
for all $h \in \mathbb{R}^n$.
That is, with $h = \sum_{i=1}^n h_i e_i$ we have 
 \[ 
    \left( \begin{array}{c}
         df_1(x)h\\
	       \vdots\\
	             df_m(x)h 
		         \end{array} \right) 
			 =
			 \left( \begin{array}{c c c}
			       \partial_1 f_1(x) & \ldots & \partial_n f_1(x)\\
			             \vdots & \ddots & \vdots\\
				           \partial_1 f_m(x) & \ldots & \partial_n f_m(x)
					       \end{array} \right)
			    \left( \begin{array}{c}
			          h_1\\
				        \vdots\\
					     h_n
					          \end{array} \right) \,.\]
						  In other words, the Jacobian matrix $Df(x)$ is the
						  representation of $df(x)$ with respect to the 
						  given basis $e_1, \ldots, e_n$.



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}
\begin{proof}
It suffices to prove
the statement for $m=1$. Continuity of $f$ in $x$ follows from
\[
\lim_{h \to 0}\big (f(x+h)-f(x)\big)
= \lim_{h \to 0} \big( Lh - R_f(h)\big)=0.
\]
To show that the partial derivatives exist,  choose $h=te_i$. Then differentiability of $f$ in $x$ implies
\[
\Big|
\frac{1}{t} \big ( f(x+t e_i) - f(x) \big ) - L e_i \Big| \to 0 \qquad \text{ as } t \to 0.
\]
Hence $\partial_i f(x) = L e_i$. 
Since $h=\sum_{i=1}^n h_i e_i$ we find $Lh= \sum_{i=1}^n h_i Le_i = \sum_{i=1}^n h_i \partial_i f(x)$.
\end{proof}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}


\begin{itemize}
\item[a)]
Proposition in particular implies that if 
 $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable
in $x \in \Omega$, then
\[
df(x)h= \sum_{i=1}^n \partial_i f(x) h_i = \langle \nabla f(x), h\rangle\,.
\]
This can also be seen as the definition of the gradient. The gradient is the vector field
$g$ such that $Lh = \langle g, h\rangle$ for all $h \in \mathbb{R}^n$.
\item[b)]
The converse of Proposition \ref{P.totalpartial} is not true, as we have seen before in Example \ref{E.notcontinuous}.
\end{itemize}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{A Sufficient condition for  differentiability}

It is often not so easy to use the definition of differentiability to decide whether
a function is differentiable or not. The following result gives a useful criterion.

\begin{block}{Continuous partial derivatives imply differentiability}

Suppose that the function
   $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m$
  has continuous partial derivatives.
Then $f$ is differentiable in $\Omega$.
\end{block}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{A Sufficient condition for  differentiability}

\begin{proof}
It suffices to consider the case $m=1$.
Let 
  $h \in \mathbb{R}^n$; a candidate for the derivative is 
  $Lh = \sum_{k=1}^n \partial_k f(x) h_k$.\\
Let 
  $x_0 = x, x_k = x+ \sum_{j=1}^k h_j e_j$, such that $x_n = x + h$.
Then
  $f(x_k) - f(x_{k-1}) 
  = f(x_{k-1} + h_k e_k) - f(x_{k-1})$ and
 the Mean Value Theorem (for functions of one variable) implies
$f(x_k) - f(x_{k-1})
  =\partial_k f(x_{k-1} + \theta_k h_k e_k)h_k$ with $\theta_k \in [0,1].$

\end{proof}

\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{A Sufficient condition for  differentiability}

% \begin{proof}

  % Hence
% \begin{eqnarray*}
    % {|f(x+h) - f(x) - Lh| \over |h|}
    % & = & {1 \over |h|}  
    % \left| \sum_{k=1}^n (f(x_k) - f(x_{k-1})) -\sum_{k=1}^n \partial_k f(x) h_k\right|\\
    % & = & {1 \over |h|} 
    % \left| \sum_{k=1}^n \partial_k f(x_{k-1} + \theta_k h_k e_k)h_k
      % - \sum_{k=1}^n \partial_k f(x) h_k \right|\\
    % & \le & {|h| \over |h|} 
    % \left( \sum_{k=1}^n \Big( \partial_k f(x_{k-1} + \theta_k h_k e_k)
      % - \sum_{k=1}^n \partial_k f(x)\Big)^2 \right)^{1 \over 2}\\
    % & \rightarrow & 0 \quad \text{ f  \quad r } h \rightarrow 0,
  % \end{eqnarray*}
 % since 
   % $\partial_k f$ is continuous in  $x$.
% \end{proof}

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{A Sufficient condition for  differentiability}

\begin{corollary}
If $f$  has continuous partial derivatives, then $f$ is continuous.
\end{corollary}

\end{frame}




% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{The Chain Rule}

% \begin{block}{Proposition}(Chain Rule)

% Let
   % $\Omega \subseteq \mathbb{R}^n$ and $ V \subseteq \mathbb{R}^m$ be
   % open and connected sets, 
 % let  $g \colon \Omega \to V$ 
   % and $f \colon V \to \mathbb{R}^k$.
 % Suppose that  
  % $g$ is differentiable in  $x \in \Omega$  
  % and $f$ is differentiable  in $y = g(x) \in V$.
% Then the map
   % $f \circ g\colon \Omega \rightarrow \mathbb{R}^k$ 
  % is differentiable in  $x$ and
  % \[ d(f \circ g)(x) = d f(g(x)) d g(x)\,.\]
  % In coordinates this reads 
  % \[
  % \frac{\partial}{\partial x_j}(f_i \circ g)(x)
  % = \sum_{l=1}^m \frac{\partial}{\partial y_l} f_i(g(x)) 
  % \frac{\partial}{\partial x_j} g_l(x)\,, \qquad i=1,\ldots,k\,, \quad j=1,\ldots, n\,. \]
% \end{block}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{The Chain Rule}

% \begin{proof}
 % We define
 % $A := d g(x)$ and 
  % $B := d f(g(x))$.
  % We need to show that  $d(f \circ g)(x) = B  A$.\\
% Since 
   % $g$ and $h$ are differentiable, we have
  % \[g(x+h) = g(x) + Ah + R_g(h) 
  % \quad \text{with } h \in \mathbb{R}^n, R_g(h) = o(|h|)\]
  % and 
  % \[f(y+\eta) = f(y) + B\eta + R_f(\eta)
  % \quad \text{ with } \eta \in \mathbb{R}^m, R_f(\eta) = o(|\eta|).\]

 % We choose now $\eta := g(x+h) - g(x) = Ah + R_g(h)$. Then
  % \[ f(g(x+h)) = f(g(x)) + B(Ah + R_g(h)) + R_f(Ah + R_g(h))\,.\]

 
% \end{proof}
% %\begin{example}
% %  $k=1: \partial_j ((f\circ g)(x)) 
% %  = \sum_{l=1}^n \partial_k f(g(x)) \partial_j g_l(x)
% %  = ((D g(x))^T \nabla f(g(x)))_j$
% %\end{example}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{The Chain Rule}

% \begin{proof}
 

  % It remains to show $g(h) := B R_g(h) + R_f(Ah + R_g(h))= o(|h|)$
% To that aim notice that 
  % \[{|B  R_g(h)| \over |h|}
  % \le {|B| |R_g(h)| \over |h|}
  % \rightarrow {0} \text{ for } |h| \rightarrow 0\]
  % and, for sufficiently small $|h|$,  
  % \[|A h+ R_g(h)| 
  % \le |A|  |h| + |R_g(h)|
  % \le (|A| + 1) |h|.\]
% Hence
  % \[ {R_f( A  h + R_g(h)) \over |h|}
  % \rightarrow 0 \text{ for } |h| \rightarrow 0 \]
% \end{proof}
% %\begin{example}
% %  $k=1: \partial_j ((f\circ g)(x)) 
% %  = \sum_{l=1}^n \partial_k f(g(x)) \partial_j g_l(x)
% %  = ((D g(x))^T \nabla f(g(x)))_j$
% %\end{example}
% \end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Derivative of the Inverse}


  Suppose that
$f\colon \mathbb{R}^n \rightarrow \mathbb{R}^n$ is invertible with inverse
  $g\colon \mathbb{R}^n \rightarrow \mathbb{R}^n$. 
Suppose further that 
  $f$ is differentiable in  $x$ and that
 $g$ is differentiable  in $g=f(x)$.\\
Then the Chain Rule implies for 
  $g(f(x)) = x$ that
  \[ d g(f(x)) d f(x) = \text{Id}
  \qquad \text{ and hence } \qquad d g(f(x)) = (d f(x))^{-1}\,. \]


\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{Polar coordinates in $\mathbb{R}^2$}

  
  % Let $f\colon \mathbb{R}_+ \times (0, 2\pi) \rightarrow \mathbb{R}^2$
  % be given by  $f(r, \varphi) = (r \cos \varphi, r \sin \varphi) =: (x,y)$.
 % Let $g$ be the inverse function to $f$.
% From
  % \[
  % D f(r, \varphi) 
  % = \left( \begin{array}{c c}
      % \cos \varphi & -r \sin \varphi\\
      % \sin \varphi & r \cos \varphi
    % \end{array} \right)
  % \]
  % we deduce that  $\det D f(r, \varphi) = r > 0$. Then\\
  % \[
  % Dg(x,y) = D f(r, \varphi)^{-1}
  % = \left( \begin{array}{c c}
      % \cos \varphi & \sin \varphi \\
      % - {1 \over r} \sin \varphi & {1 \over r} \cos \varphi
    % \end{array} \right)
  % = \left( \begin{array}{c c}
      % {x \over \sqrt{x^2 + y^2}} & {y \over \sqrt{x^2 + y^2}}\\
      % -{y \over x^2 + y^2} & { x \over x^2 + y^2}
    % \end{array} \right)
  % \]


% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{The gradient is perpendicular to level sets}


% Let
  % $f\colon \Omega \subseteq \mathbb{R}^n 
  % \rightarrow \mathbb{R}$ be differentiable and let 
  % $\gamma$ be a regular curve $(\alpha, \beta) \rightarrow \Omega$, 
% which lies in a level set of $f$, that is
   % $f(\gamma(t)) = c $ for all $  t \in (\alpha, \beta)$.
  % Then we have for all $t \in (\alpha, \beta)$ that
% \[ 
% 0 = df(\gamma(t)) \gamma'(t) =
% \left< \nabla f(\gamma(t)), \gamma'(t) \right> 
  % \]



% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{The gradient is perpendicular to level sets}

% The direction of  $\nabla f(x)$ is  the direction 
% of steepest ascent in $x$, 
  % ($- \nabla f(x))$ is the direction of steepest
% descent.
% Indeed, consider any 
  % $ v \in \mathbb{R}^n$ with $|v| = 1$. Then
% \[ \left<\nabla f(x), v \right> 
  % \le |\nabla f(x)| |v|
  % = |\nabla f(x)| 1
  % = |\nabla f(x)|\]
 % and equality holds if   
  % $v = {\nabla f(x) \over |\nabla f(x)|}$.

% \end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{Mean Value Theorems}

% Our goal in this section is to  use information about the derivative of a function  to obtain information
% about the function itself.

% \begin{block}{Remark}
  % In the case  $n=1$ we know the following Mean Value Theorem
% for a differentiable function $f$:
  % $f(x) - f(y) = f'(\xi)(x-y)$ for some $\xi \in (x,y)$.
% We cannot generalize this, however, for vector-valued
% functions, since in general we get a different $\xi$ for
% every component.
% The Fundamental Theorem of Calculus does not have this disadvantage:
  % $f(y) - f(x) = \int_x^y f'(\xi) \dop \xi$ 
% is also true for vector-valued functions, but needs that $f'$ is
% integrable.
% \end{block}

% We are now going to prove some versions of the Mean Value Theorem for functions
% of several variables.
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{}
% \begin{block}{Proposition}

  % Suppose that
 % $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable
% and let  
   % $x,y \in \Omega$, 
  % such that also $[x;y]:=\{ tx + (1-t)y  \,|\, t \in [0,1]\} 
  % \subset \Omega$.
% Then there exists 
   % $\xi \in [x;y]$, 
% such that
  % \[ f(x) - f(y) = d f(\xi)(x-y)
  % = \left< \nabla f(\xi), x-y \right> \]
% \end{block}
% \begin{proof}
  % Let $\gamma(t) = tx+(1-t)y, t \in [0,1]$, and $ F(t) = f(\gamma(t))$. Then
   % $f(x) = F(1)$ and $f(y) = F(0)$.
% The Chain Rule implies that $f$ is differentiable and 
  % \[
  % {\dop\over \dt} F(t) = d f(\gamma(t)) \gamma'(t)\,.\]
% By the Mean Value Theorem for $n=1$ there exists 
 % $\tau \in (0,1)$, 
% such that $F(1) - F(0) = F'(\tau)$. Hence
  % \[f(x) - f(y) = d f(\gamma(\tau)) (x-y) = d f(\xi) (x - y) \qquad \text{ with } \xi= \gamma(\tau) \,.\]
% \end{proof}
% \begin{corollary}
% Let $\Omega \subseteq \mathbb{R}^n$ be path connected. If then  
   % $f\colon \Omega \rightarrow \mathbb{R}$ satisfies  $d f(x) = 0$ for
% all $x \in \Omega$ then $f$ is constant in $\Omega$. 
% \end{corollary}
% \begin{proof}
% Connect two arbitrary points by a polygon and apply the Mean Value
% Theorem to each part.
% \end{proof}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{}
% \begin{block}{Proposition}
  % \label{MWS2}

 % Let $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m, 
  % f \in C^1(\Omega, \mathbb{R}^m)$.
  % Suppose that for  $x,y \in \Omega$ there exists a regular curve  
  % $\gamma: [\alpha, \beta] \rightarrow \Omega$ which connects $x$ and $y$, i.e  
  % $\gamma(\alpha) = y, \gamma(\beta) = x$. Then
% \[ f(x)-f(y)
  % = \int_\alpha^\beta d f(\gamma(t))  \gamma'(t) \dt \,.\]
% \end{block}
% \begin{proof}
% Use the Fundamental Theorem of Calculus  and the Chain Rule for 
  % for $F(t) = f(\gamma(t))$. 
% \end{proof}

% \begin{block}{Remark}
% Another version: let
   % $x \in \Omega, \xi \in \mathbb{R}^n$ und $\forall t \in [0,1]: x + t \xi \in \Omega$. Then
  % \[ f(x+\xi) - f(x) = \int_0^1 d f(x + t\xi) \xi  \dt\,. \]

% \end{block}
% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{}
% \begin{block}{Proposition}
% \label{MWS3}

  % Let  $\Omega \subseteq \mathbb{R}^n$ be open and convex,
% i.e. for all points in $x,y \in \Omega$ we also have $[x;y] \subset \Omega$.
% Suppose that 
   % $f \in C^1(\Omega, \mathbb{R}^m)$ 
  % and $\sup_{x \in \Omega} |
% D f(x) | \le K$. 
  % Then we have for all  $x,y \in \Omega$  that
  % \[|f(x) - f(y)| \le K |x-y|\,, \]
  % that is $f$ is Lipschitz continuous in $\Omega$
  % with Lipschitz constant $K$.
% \end{block}
% \begin{proof}
% Homework.
% \ignore{
   % For continuous
    % $\sigma: [a,b] \subseteq \mathbb{R} \rightarrow \mathbb{R}^n$ we have
   % $ 
      % \left| \int_a^b \sigma(t) \dt\right| \le \int_a^b |\sigma(t)| \dt. $
% By assumption $\gamma(t) = tx + (1-t) y \in \Omega
    % $ for all $t \in [0,1]$. Proposition \ref{MWS2} implies
    % \begin{eqnarray*}
      % |f(x) - f(y)|
      % &=& \left| \int_0^1 d f(\gamma(t)) \gamma'(t) \dt  \right|\\
      % &=& \left| \int_0^1 D f(\gamma(t)) \gamma'(t) \dt  \right|\\
      % &\leq & \int_0^1 |D f(\gamma(t)) (x-y)| \dt\\
      % &\le& \int_0^1 |D f(\gamma(t))| |(x-y)| \dt\\
      % &\le& K  |x-y|
    % \end{eqnarray*}
% }
% \end{proof}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{The Inverse Function Theorem and the Implicit Function Theorem}

% The Inverse Function Theorem and the Implicit Function Theorem are some
% of the most important Theorems in Analysis.
% The Inverse Function
% Theorem tells us when we can locally invert a function; the Implicit
% Function Theorem  tells us when a function is given implicitly as the function
% of other variables. We will discuss both theorems in $\mathbb{R}^n$ here,
% but they are also valid in basically the same form in infinite-dimensional
% spaces (more precisely, in Banach spaces).
% Their applications are vast and we can only get a 
% glimpse  of their significance in this course.


% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{The Inverse Function Theorem and the Implicit Function Theorem}

% The theorems are equivalent; the classical approach, however, is to 
% prove first the Inverse Function Theorem via the Contraction Mapping Fixed
% Point Principle and then deduce the Implicit Function Theorem from it.
% The proof of the Inverse Function Theorem is however lengthy and technical
% and we do not have the time to go through it in this lecture course.
% We recommend the book by Krantz and Parks ({\it The Implicit Function Theorem,
% History, Theory and Applications}, Birkh\"auser) where you can also find
% an elementary (but still not short) proof of the Implicit Function Theorem which does not
% use the Inverse Function Theorem. The latter then follows directly as a corollary from 
% the Implicit Function Theorem.

% \end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{The Implicit Function Theorem in $\mathbb{R}^2$}

% We start with a simple example. Consider $S^1:=\{ (x,y) \in \mathbb{R}^2 \, |\, x^2+y^2 =1 \}$,
% which is just the unit circle in the plane. Can we find a function $y=y(x)$ such that
% $x^2 + y(x)^2=1$? Obviously, in this example, we cannot find one function to describe
% the whole unit circle in this way. However, we can do it {\it locally}, that is in a neighbourhood
% of a point $(x_0,y_0) \in S^1$, as long as $y_0 \not= 0$. In this example we can find $y$ explicitly: it is
% $y(x)=\sqrt{1-x^2}$ if $y_0>0$ and $y(x)=-\sqrt{1-x^2}$ if $y_0<0$ both for $|x|<1$.
% Notice also, that if
 % $y_0 =0$, we cannot find such a function $y$, but we can instead write $x$ as a function of $y$.

% The Implicit Function Theorem describes conditions under which  certain variables can be written as
% functions of the others. In $\mathbb{R}^2$ it can be stated as follows.

% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{Implicit Function Theorem in $\mathbb{R}^2$}

% Let $\Omega \subseteq \mathbb{R}^2$ be open and $F \in C^1(\Omega)$. Let $(x_0,y_0) \in \Omega$ and assume that
% \[
% f(x_0,y_0)=0 \qquad \text{ and } \qquad \frac{\partial f}{\partial y}(x_0,y_0) \not=0\,.
% \]
% Then there exist open intervals $I, J \subseteq \mathbb{R}$ with $x_0 \in I$, $y_0 \in J$ and a unique function
% $g\colon I \to J$ such that $y_0=g(x_0)$ and 
% \[
% f(x,y)=0 \qquad \text{ if and only if } \qquad y=g(x) \quad \text{ for all } (x,y) \in I\times J.
% \]
% Furthermore, $g \in C^1(I)$ with
% \begin{equation}\label{psiderivative}
% g'(x_0) = - \frac{\frac{ \partial f}{\partial x}(x_0,y_0) }{
% \frac{ \partial f}{\partial y}(x_0,y_0)} \,.
% \end{equation}



% Obviously, an analogous result is true if $\frac{\partial f}{\partial x} (x_0,y_0) \not=0$.


% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}[fragile] \frametitle{}
%%\begin{proof} (Not examinable)
%%Without loss of generality we can assume that $\frac{ \partial f}{\partial y}(x_0,y_0)>0$.
%%Due to the continuity of $\frac{ \partial f}{\partial y}$ we can also assume -- by making $\Omega$ smaller
%%if necessary -- that 
%%\begin{equation}\label{proof1}
%%\frac{ \partial f}{\partial y} (x,y)\geq \delta >0 \qquad \text{ for all } (x,y) \in \Omega\,.
%%\end{equation}
%%As a consequence we can find $y_1< y_0 < y_2$ such that
%%$f(x_0,y_1) < 0 < f(x_0,y_2)$ and due to the continuity of $f$ we can find an open interval $I$ containing
%%$x_0$ such that 
%%\begin{equation}\label{proof2}
%%f(x,y_1) < 0 < f(x,y_2) \qquad \text{ for all } x \in I\,.
%%\end{equation} 
%%The Intermediate Value Theorem and \eqref{proof1} imply that for each $x \in I$ there exists a unique
%%$y \in (y_1,y_2)=:J$ such that $f(x,y)=0$. Denote this $y$ by $g(x)$.
%%The continuity of $f$ and the uniqueness of $y$ also imply that $g$ is continuous.
%%
%%To complete the proof of the theorem we need to show that $g$ is continuously differentiable in $I$
%%and that \eqref{psiderivative} holds.
%%With the notation  $y=g(x)$
%%we find
%%\begin{equation}\label{proof3}
%%f(x+s,y+t) - f(x,y) = s \frac{\partial f}{\partial x}(x,y) + t \frac{\partial f}{\partial y}(x,y) 
%%+ \varepsilon(s,t) \sqrt{s^2+t^2}\,,
%%\end{equation}
%%with $\varepsilon(s,t) \to 0$ as $(s,t) \to 0$.
%%We now choose $t= g(x+s)-g(x)$ such that the left hand side in 
%%\eqref{proof3} vanishes and  obtain
%%\begin{equation}\label{proof4}
%% t \frac{\partial f}{\partial y}(x,y) = -  s \frac{\partial f }{\partial x}(x,y) - \varepsilon(s,t) \sqrt{s^2+t^2} \,.
%% \end{equation}
%% We rearrange to obtain
%% \[
%% \Big | \frac t s + \frac{\partial f }{\partial x}(x,y)/\frac{\partial f}{\partial y}(x,y)\Big |
%% \leq \frac{|\varepsilon|}{ | \frac{\partial f}{\partial y}(x,y)|} \Big ( 1 + \frac{|t|}{|s|}\Big)\,.
%% \]
%% Thus, if we can show that $ \frac{|t|}{|s|} \leq C$ as $ s \to 0$, then we can let $s\to 0$ in the above
%% inequality to find that  indeed $g'(x)$ exists for all $x \in I$. For $(x,y)=(x_0,y_0)$ we  find the 
%% formula in \eqref{psiderivative} and the properties of $f$ and $g$ also imply that $g'$ is continuous.
%%
%% We still need to show that $ \frac{|t|}{|s|} \leq C$ . We obtain from \eqref{proof4} that
%% \[
%%  \frac{|t|}{|s|} \leq \big | \frac{\partial f }{\partial x}(x,y)\big |/ \big| \frac{\partial f}{\partial y}(x,y)
%%  \big |  + \frac{|\varepsilon|}{  \big| \frac{\partial f}{\partial y}(x,y)
%%    \big |} \Big ( 1 +  \frac{|t|}{|s|} \Big )\,
%%    \leq \big | \frac{\partial f }{\partial x}(x,y)\big |/ \big| \frac{\partial f}{\partial y}(x,y)
%%      \big |  + \frac{|\varepsilon|}{ \delta }
%%          \Big ( 1 +  \frac{|t|}{|s|} \Big )\,.
%%    \]
%%We can choose now $|s|$ so small such that $\frac{|\varepsilon|}{\delta} \leq \frac 1 2 $ and then
%%\[
%%\frac{|t|}{|s|} \leq  2 \big | \frac{\partial f }{\partial x}(x,y)\big |/ \big| \frac{\partial f}{\partial y}(x,y)
%%  \big |  + 1\,, \]
%%  which finishes the proof of the theorem.
%%\end{proof}
%%
%%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{}
%
%We show that for sufficiently small $a>0$ there exists a function $g \in C^1(-a,a)$ with
%$g (0)=0$ such that
%\[
%g^2(x)x + 2 x^2 e^{g(x)} = g(x)\,.
%\]
%Indeed, define $f\colon \mathbb{R}^2 \to \mathbb{R}$ via $f(x,y) = y^2 x + 2x^2 e^y - y$. Then
% $f(0,0)=0$ and $\partial_y f(0,0)=-1$. Hence the Implicit Function Theorem implies the existence
% of  the function $g$ as claimed.
% Furthermore we can compute $g'(0) = - \partial_x f(0,0)/\partial_y f(0,0)=0$.
%
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{The Implicit Function Theorem in $\mathbb{R}^n$}
%
%To formulate the Implicit Function Theorem in $\mathbb{R}^n$ we first introduce some convenient notation.
%We write
%  \begin{itemize}
%  \item $ \mathbb{R}^n = \mathbb{R}^k \times \mathbb{R}^m 
%    \ni (x_1, \ldots, x_k, y_1, \ldots y_m) =: (x,y)$
%  \item $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m, 
%    (x_0, y_0) \in \Omega \subseteq \mathbb{R}^n, f(x_0, y_0) =:z_0$
%    \end{itemize}
%
%  We are looking for open neighbourhoods  $U$ of $x_0$ and $V$ of $y_0$
%  as well as a function 
%     $g\colon U \rightarrow V$ such that
%    \[ \forall (x,y) \in U \times V: 
%    f(x,y) = z_0  = f(x_0,y_0)  \Leftrightarrow y = g(x) \]
%    
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{}
%It is instructive to consider first the linear case.
%  Let $f(x,y) = Ax + By$ with $ A \in M_{m\times k}(\mathbb{R})$ and $  B \in M_{m\times m}(\mathbb{R})$.
%If $B$ is invertible then the equation $f(x,y) = Ax_0+B y_0=: z_0$ can be solved for $y$  via
%  \[
%  y 
%  =  B^{-1} \big ( z_0 - Ax \big).
%  \]
%
%  Now consider the nonlinear case. Let $f\in C^1(\Omega)$ and  write
%  \[
%  D f(x,y) ) 
%    = (D_x f(x,y), D_y f(x,y))\,,\]
%    where
%    \[ D_x f(x,y) 
%    = \left( {\partial f_j \over \partial x_i} \right)
%    \in M_{m\times k} (\mathbb{R}) \qquad (j=1,\ldots, m; i=1,\ldots, k) \]
%    and
%    \[ D_y f(x,y) 
%    = \left( {\partial f_j \over \partial y_i} \right)
%    \in M_{m \times m} (\mathbb{R}) \qquad (j=1,\ldots, m; i=1,\ldots, m)\,. \]
%
%    
%    \end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{}
%
%    Then
%    \[
%      f(x,y) 
%      =  f(x_0, y_0) 
%      + D_x f(x_0, y_0)(x-x_0) + D_y f(x_0, y_0) (y - y_0) 
%      + o(|(x,y) -(x_0,y_0)|)
%      \]
%      and thus
%     $ f(x,y) \approx f(x_0,y_0)= z_0$  
%      if 
%      \[
%      D_x f(x_0, y_0)(x - x_0) \approx -D_y f(x_0, y_0)(y - y_0) \,.
%      \]
%  If $D_y f(x_0, y_0)$ is invertible we find
%    \[ y \approx y_0 - \Big(D_y f(x_0, y_0)\Big)^{-1}  D_x f(x_0, y_0)(x - x_0).\]
%    
%    \end{frame}
%    
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{The Implicit Function Theorem}
%
%   Let $f\colon \Omega \subseteq \mathbb{R}^{k+m} \rightarrow \mathbb{R}^m$, where  $n=k+m$, 
%   $f  \in C^1(\Omega,\mathbb{R}^m)$ 
% and  let  $(x_0, y_0) \in \Omega$ with  $z_0 := f(x_0, y_0)$.
%  If $D_y f(x_0, y_0)$ is invertible  then there exist
%  open neighbourhoods 
%   $U$ of $x_0$ and $V$ of $y_0$, 
%  and a function  $g \in C^1(U,V)$ such that
%  \[ \{ (x,y) \in U \times V \mid f(x, y) = z_0 \}
%  = \{ (x,y) \mid x \in U, y = g(x) \}. \]
%  Furthermore 
%  \[
%  Dg(x_0) = - 
%  \Big ( D_yf(x_0,y_0)\Big)^{-1} D_x f(x_0,y_0) \,.
%  \]
%  
%  \end{frame}
%  
%  
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{The Implicit Function Theorem}
%
%(Nonlinear system of equations)
%
%Consider the system of equations
%  \[f(x, y_1, y_2)
%  = \left( \begin{array}{c}
%      x^3 + y_1^3 + y_2^3 -7\\
%      xy_1 + y_1 y_2 + y_2 x + 2
%    \end{array} \right)
%  = \left( \begin{array}{c} 0\\ 0 \end{array} \right).\]
%The function  $f$ is zero in the point
%   $(2,-1,0)$ and 
%  \[D f(x,y_1, y_2) = \left( \begin{array}{c c c}
% 3x^2 &    3y_1^2 & 3y_2^2 \\
%    y_1+y_2 &  x+ y_2 &  x + y_1
%    \end{array} \right), \]
%    hence 
%    \[
%  D_y f(2, -1, 0) = \left( \begin{array}{c c}
%      3 & 0\\
%      2 & 1
%    \end{array} \right) \qquad \text{ with } \quad \text{det} D_yf(2,-1,0) =3 \not=0.\]
%  \end{frame}
%  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile] \frametitle{The Implicit Function Theorem}
%
%
%\begin{itemize}
%\item [a)] 
%    The Implicit Function Theorem implies that there exist 
%    open neighbourhoods 
%  $I$ of 2 and $V \subseteq \mathbb R^2$ of  $(-1,0)$ 
%  and a continuously differentiable function
%   $g\colon I \rightarrow V$, with $g(2)=(-1,0)$,  
%   such that
%  \[
%  f(x, y_1, y_2) = 0 
%  \quad \Leftrightarrow \quad
%  y = (y_1, y_2) = g(x) = (g_1(x), g_2(x))
%  \]
%  for all $x \in I, y \in V$.
%  Furthermore, the derivative of $g$ in $x_0=2$ is given by
%  \[
%  Dg(2) = - \left( \begin{array}{c c}
%        3 & 0\\
%	      2 & 1
%	          \end{array} \right)^{-1} \left( \begin{array}{c }
%		       12 \\
%			      -1 
%			          \end{array} \right) 
%				  = - \frac 1 3 \left( \begin{array}{c c}
%				        1 & 0\\
%					   -   2 & 3
%					          \end{array} \right) 
%						  \left( \begin{array}{c }
%						                         12 \\
%									                               -1
%												                                         \end{array} \right)
%     = \left( \begin{array}{c }
%     -4 \\ 9 \end{array} \right)\,.
%     \]
%
%\item[b)]
%The function  $f\colon \mathbb{R}^4 \to \mathbb{R}^2$ is given by
%\[
%f(x,y,u,v) = \left ( \begin{array}{c} x^2 + uy + e^v \\ 2x +u^2 - uv \end{array} \right )\,.
%\]
%Consider the point $(2,5,-1,0)$ such that $f(2,5,-1,0) = (0,5)^T$. The Jacobian matrix of $f$ is 
%\[
%Df(x,y,u,v) = \left ( \begin{array}{ c c c c} 2x & u & y & e^v \\ 2 & 0 & 2u-v& -u \end{array} \right )\,.
%\]
%Hence 
%\[
%D_{(u,v)} f(x,y,u,v) = \left( \begin{array}{c c} y & e^v\\ 2u-v & - u \end{array} \right)
%\quad \text{ and } \quad D_{(u,v)} f(2,5,-1,0) = \left( \begin{array}{c c} 5 & 1 \\ -2  & 1 \end{array} \right)\,.
%\]
%Since $J_f(2,5,-1,0) = 7 \not=0$, the
%Implicit Function Theorem implies that 
%there exist open neighbourhoods $U \subset \mathbb{R}^2$ of $(2,5)$ and $V \subset \mathbb{R}^2$ of $(-1,0)$ 
%and
%a function $g \in C^1(U,V)$ with $g(2,5) = (-1,0)$ and $f(x,y,g(x,y)) = (0,5)^T$ for all $(x,y) \in U$.
%We can also compute that 
%\[
%Dg(2,5) = - \left ( \begin{array}{c c} 5&1\\ -2& 1 \end{array} \right)^{-1} 
%\left ( \begin{array}{c c} 4& -1 \\ 2 & 0 \end{array} \right) = - \frac 1 7 
%\left ( \begin{array}{c c } 2& -1 \\ 18 & -2 \end{array} \right )\,.
%\]
%
%
%
%  \item[c)] (Write surface locally as graph)
%  
%  Let $h\colon \mathbb{R}^3 \to \mathbb{R}$ with $h(x,y,z) = xy - z \log y + e^{xz} -1$. Can we represent
%  the 'surface' given by $h(x,y,z)=0$ locally in a neighbourhood of $(0,1,1)$ either in the form
%  $x=f(y,z)$, $y=g(x,z)$ or $z=p(x,y)$? The Jacobian matrix  of $h$ is $D h(x,y,z)= (y+ze^{xz},
%  x-\frac{z}{y}, - \log y + xe^{xz})$ and thus $Dh(0,1,1)=(2,-1,0)$. Hence, the Implicit Function
%  Theorem tells us that we can represent the surface locally as $x=f(y,z)$ or $y=g(x,z)$, but it does not
%  tell us whether we can do it in the form $z=p(x,y)$. In fact, one can show that the latter is not
%  possible. 
%  \end{itemize}
%
%  
%  \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{The Inverse Function Theorem}


In this section we consider the following problem. Given a function
  $f\colon \Omega \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^n$, does there exist
  locally around a point $x_0$ an inverse function
       $g = f^{-1}$?

The idea is, as usual, to linearise around a point.
	  Let  $x_0 \in \mathbb{R}^n , y_0 = f(x_0)$ and assume that the Jacobian matrix $D f(x_0)$
	  is invertible. Then we find for 
	     $x \approx x_0$ that 
	        \[y = f(x) = y_0 + D f(x_0)(x - x_0) + o(|x-x_0|)\,,\]
		    that is \[
		      y \approx f(x)
		        \quad \text{ if } \quad
			  x \approx x_0 + D f(x_0)^{-1} (y - y_0).
			    \]

		
\end{frame}
		
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 % \begin{frame}[fragile] \frametitle{The Inverse Function Theorem}
 
% % Let $f\colon \mathbb{R} \rightarrow \mathbb{R}$ be given by $f(x) := x^2$.

    % % \begin{itemize}
      % % \item For $x_0 > 0$ or $x_0 <0$   we have that  $f$ is invertible in a neighbourhood
      % % of $x_0$
	  % % \item For $x_0 = 0$ there is no neighbourhood of $x_0$ where $f$ 
	  % % has an inverse. Indeed, 
	      % % $f'(0) = 0$ is not invertible
	        % % \end{itemize}


		
% \end{frame}		

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{Diffeomorphism}


    Let  $f \colon U \subseteq \mathbb{R}^n \rightarrow V \subseteq \mathbb{R}^n$ and suppose that
      $U$ and $ V$ are open in  $\mathbb{R}^n$. We say that $f$ is a diffeomorphism if 
	       $f$ is bijective, that is there exists  $f^{-1}: V \rightarrow U$, and if
	         $f \in C^1(U,V)$ and $ f^{-1} \in C^1(V,U)$.

		    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}

    Here is a simple example for a function $f\colon (-1,1) \to (-1,1)$ which is bijective,
    $f \in C^1$, but $f^{-1}$ is not differentiable on $(-1,1)$.

        Let $f\colon (-1, 1) \rightarrow (-1,1)$ be given by  $f(x) = x^3$.
	Obviously $f$ is bijective with inverse $f^{-1}  \colon (-1, 1) \rightarrow (-1,1)
	$ given by
		 $ f^{-1}(y) = y^{1 \over 3}$. Furthermore, $f \in C^{\infty}(-1,1)$, but 
		 $f^{-1}$ is not differentiable in any neighbourhood of $0$. Hence, $f$ is not a
		 diffeomorphism.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{The Inverse Function Theorem in $\mathbb{R}^n$}

		  
		 Let $\Omega \subseteq \mathbb{R}^n$ be open, let  $f \in C^1(\Omega, \mathbb{R}^n)$ 
		 and let $x_0 \in \Omega$.
		      If  $D f(x_0)$  is invertible, then there exists an  open neighbourhood
		      $U$ of $x_0$ such that $f(U)$ is open 
and		      $f\colon U \to f(U)$ is a diffeomorphism.
Furthermore \[Df^{-1}(f(x_0)) = \big ( Df(x_0) \big)^{-1}\,.\]



				Notice that it follows that $f^{-1} \in C^1$ in a neighbourhood of $f(x_0)$, 
				we do not need to assume it.

				
				 $f\colon \mathbb{R} \to \mathbb{R}, f(x)=x^2$.
    If  $x_0 > 0$ we can choose  $U =  (0, \infty)$, if 
       $x_0 < 0$ we can choose  $U = (- \infty, 0)$.
				\end{frame}
				
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}

  Let $f\colon \mathbb{R}_+ \times \mathbb{R} \rightarrow \mathbb{R}^2, $
  be given by 
      \[( r, \varphi ) \mapsto (r \cos \varphi, r \sin \varphi)\]
        \[D f(r, \varphi) = \left( \begin{array}{c c}
	      \cos \varphi & -r \sin \varphi \\
	            \sin \varphi & r \cos \varphi
		        \end{array} \right)
			  \quad \text{ such that } \quad \det D f(r, \varphi) = r > 0.\]
			  We have already seen that
			    \[(D f(r, \varphi))^{-1} = {1 \over r} \left( \begin{array}{c c}
			          r \cos \varphi & r \sin \varphi \\
				      -  \sin \varphi & \cos \varphi
					    \end{array} \right).\]
					    Hence $f$ is locally invertible everywhere, but not globally.
					    The local inverse can be computed:
						  Let $f(r, \varphi)=: (x,y) \in \mathbb{R}^2$:
						  and let
						      \[U = \left\{ (r, \varphi) \mid \varphi \in \left( - {\pi \over 2}, {\pi \over 2}\right) \right\}
						        \quad \text{and} \quad
							  V = \{ (x,y) \in \mathbb{R}^2 \mid x > 0 \}.\]

							    $f\colon U \rightarrow V$ is a diffeomorphism, where
							     $g = f^{-1}\colon V \rightarrow U$ is given by
							        \[g(x,y) = \left(\sqrt{x^2 + y^2}, \arctan {y \over x}\right).\]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}[fragile] \frametitle{}

Let $f \colon (0,\infty) \times \mathbb{R} \to \mathbb{R}^2$ be given by
$f(x,y) := (\cosh x \cos y, \sinh x \sin y)=:(u,v)$.
Then 
\[
Df(x,y) = \left ( \begin{array}{c c} \sinh x \cos y & - \cosh x \sin y \\ \cosh x \sin y & \sinh x \cos y
\end{array} \right ) \,.
\]
Hence $J_f(x,y) = \sinh^2 x + \sin^2 y$ and thus $J_f(x,y)>0$ for all $x>0, y \in \mathbb{R}$. 
As a consequence of the Inverse Function Theorem we have that $f$ is locally a diffeomorphism for all $(x,y)$.
(The function $f$  is not a global diffeomorphism as it is periodic in $y$.) 

Notice that for fixed $x>0$ the image $f(x,y)$ describes an ellipse with axes of length $\cosh x >1$ and
$\sinh x$ respectively.
Hence $f((0,\infty) \times \mathbb{R}) = \mathbb{R}^2 \setminus \{ (u,0) \, |\, |u| \leq 1 \}$.

\end{frame}
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}[fragile] \frametitle{Submanifolds in $\mathbb{R}^n$ and constrained minimisation problems}
%%
%%
%%As a motivation for part of the material in  the present section  we 
%%consider a simple constrained minimisation problem.
%%Let  $(x,y) \in \mathbb R^2$ 
%%und $f,g \in C^1(\mathbb R^2)$.
%%Our goal is to 
%%minimise (or  maximise) $g$ under the constraint that  $f(x,y) = 0$.
%%
%%  We can often (under some  assumptions on $f$)
%%  think of the set $\Gamma:=\{ (x,y) \in \mathbb R^2 \mid f(x,y) = 0 \}$
%%  as a curve in $\mathbb{R}^2$.
%%  Let 
%%   $(x_0, y_0)$  be such that for some $\varepsilon >0$ and all 
%%     $(x,y) \in \Gamma \cap B_{\varepsilon}(x_0,y_0)$:
%%    \[
%%    g(x_0, y_0) \leq g(x,y).\]
%%    Suppose that 
%% $\nabla f(x_0, y_0) \neq 0$, 
%%and assume without loss of generality that  $\partial_y f(x_0, y_0) \neq 0$.
%%The Implicit Function Theorem guarantees that we can represent 
%%     $\Gamma$ in an open neighbourhood of  $(x_0, y_0)$ 
%%     as 
%%     $(x, \varphi(x))$ for $x \in I$, where $I$ is an open interval  with $x_0 \in I, \varphi \in C^1(I)$
%%    and $\varphi(x_0) = y_0$.
%% The tangent to  $\Gamma$ at $(x, \varphi(x))$ is given by
%%    the vector $(1, \varphi'(x))$ 
%%    and since the gradient is perpendicular to the level sets  we have 
%%    \[{1 \choose \varphi'(x_0)} \perp \nabla f(x, \varphi(x)).\]
%%  Define $G(x) := g(x, \varphi(x))$ and consider the point 
%%      $(x_0, y_0)$ where  $g$ has a local minimiser on  $\Gamma$.
%%      Then, by Fermat's Theorem and the Chain Rule,
%%    \[0 = G'(x_0) = \partial_x g(x_0, y_0) + \partial_y g(x_0, y_0)
%%    \varphi'(x_0)
%%    = \left< \nabla g(x_0, y_0), {1 \choose \varphi'(x_0)} \right>.\]
%%    Hence there exists 
%%     $\lambda \in \mathbb R$, 
%%     such that
%%    \[\nabla g(x_0, y_0) = \lambda \nabla f(x_0, y_0).\]
%%
%%    \bigskip
%%    In the following we want to generalize this procedure. We first define 
%%    submanifolds in 
%%   $\mathbb R^n$ (in the above example it was the curve $\Gamma$) and the tangent
%%   space in a point and then prove
%%   a theorem which generalises the above established
%%   existence of $\lambda$, the Lagrange multiplier.
%%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}[fragile] \frametitle{Submanifolds in  $\mathbb R^n$}
%%
%%Let
%% $f\colon \mathbb R^n \rightarrow \mathbb R$ 
%% be a $C^1$-function 
%%and  $M = \{ x \in \mathbb R^n \mid f(x) = 0 \} = f^{-1}\{0\}$ its zero  set.
%%If  $D f(x_0) \not=0$ for some $x_0 \in M$, then
%%we know from the Implicit Function Theorem, that we can
%%represent $M$ in a neighbourhood of $x_0$ as a graph of a function of $n-1$ variables.
%%When $Df(x) \not=0$ for all $x \in M$, we call $M$ an $n-1$-dimensional submanifold of $\mathbb{R}^n$.
%%
%%We are now going to generalize this definition to $k$-dimensional manifolds,
%%that we can think of as $k$-dimensional smooth surfaces.
%%
%%
%%\begin{definition} (Submanifolds of $\mathbb{R}^n$)
%%\label{D.submanifold}
%%
%%Let $0 < k < n$ be an integer. A set  $M \subseteq \mathbb R^n$ is called 
%%a  \emph{$k$-dimensional submanifold} of $\mathbb R^n$, 
%%  if for every 
%%  $x_0 \in M$ 
%%  there exists an open neighbourhood
%%   $\Omega$  of $x_0$ in $\mathbb{R}^n$ and
%%  $f \in C^1(\Omega, \mathbb R^{n-k})$, 
%%  such that 
%%  \[ M \cap \Omega = f^{-1}\{0\} 
%%  \quad \text{and} \quad \text{rank} D f(x) = n-k \quad \text{ for all } x \in \Omega\,. \]
%%\end{definition}
%%
%%\begin{block}{Remark} \
%%   It suffices to require $\text{rank} D f(x) = n-k$ for all $x \in M \cap \Omega$.
%%   Indeed, if $\text{rank} D f(x) = n-k$ for an $x \in M \cap \Omega$, this means
%%   that the matrix $Df(x)$ has $n-k$ independent columns, or in other words, that
%%   the determinant of the matrix formed by these $n-k$ independent columns is nonzero.
%%   Since $Df$ is continuous and  the determinant is a continuous function,
%%   it follows that the determinant of this submatrix is also nonzero in an open neighbourhood
%%   of $x$.
%%   
%%
%%\end{block}
%%\begin{example} \
%%\begin{itemize}
%%  \item[a)] (Curves in  $\mathbb R^n$)
%%
%%    Regular curves are outside multiple points one-dimensional
%%    submanifolds of  $\mathbb R^n$.
%%    The representation via 
%%     $ t \in I \subseteq \mathbb{R} \mapsto \gamma(t) \in \mathbb{R}^n$ 
%%     is called a \emph{parametrisation} of the curve.
%%
%%\begin{itemize}
%%\item[i)] The unit circle in $\mathbb{R}^2$:
%%    \begin{itemize}
%%    \item A parametrisation is given by $\gamma\colon
%%    [0,2\pi) \to \mathbb{R}^2;  \gamma(t) = (\cos t, \sin t)$.
%%    \item A definition as the level set of a function is given by 
%%      $\{ (x,y) \in \mathbb R^2 \mid x^2 + y^2 -1 =0\}$.
%%    \item A local representation as a graph of a function is for example
%%    $y(x) = \pm \sqrt{1 - x^2}\,;
%%      \, x \in [-1,1]$.
%%    \end{itemize}
%%\item[ii)]
%%\[ M:= \{ (x,y) \in \mathbb{R}^2 \, |\, x^3-y^2-1 =0 \}
%%\]
%%defines a one-dimensional submanifold (a regular curve) in $\mathbb{R}^2$.
%%
%%    \end{itemize}
%%
%%     \item[b)]  (Ellipsoids)
%%
%%       An ellipsoid is given by
%%         %\parpic[r]{\includegraphics[width=0.186 \textwidth]{Ellipsoid.pdf}}
%%	     \[
%%	         M = \left\{
%%		       x \in \mathbb R^3 \mid
%%		             f(x) := {x_1^2 \over a^2} + {x_2^2 \over b^2} + {x_3^2 \over c^2} -1=0
%%			         \right\}.
%%				     \]
%%				         for some $a,b,c>0$. We  check that this defines a two-dimensional submanifold
%%					     of $\mathbb{R}^3$. Indeed,
%%					         \[D f(x)
%%						     = 2 \left( {x_1 \over a^2}, {x_2 \over b^2}, {x_3 \over c^2} \right)\]
%%						          and thus $D f(x) = 0$ if and only if $x=0$,  but $x=0 \notin M$.
%%
%%
%%  \item[c)] (Hyperboloid)
%%    %\piccaption{Kegel}
%%    %\parpic[r]{\includegraphics[width=0.3 \textwidth]{Kegel.pdf}}
%%%    \picskip{9}
%%    Let $n = 3$ und $k=2$. For $c \in \mathbb R\backslash\{0\}$ let
%%    \[f(x) = x_1^2 + x_2^2 - x_3^2 -c \]
%%    The hyperboloid for given $c$ is then 
%%    \[H_c = \big \{ x \in \mathbb{R}^3 \mid f(x) = 0 \big \}.\]
%%
%%    On the revision sheet  you are asked to show that this defines a two-dimensional
%%    submanifold of $\mathbb{R}^3$ if $c \not=0$. You are also asked to check
%%    what happens for $c=0$.
%%
%%\item[d)] (Torus)
%%
%%Let $n=3$ and $k=2$. For $0<r<R$ a torus is given by
%%\[
%%T:=\big \{ (x,y,z) \in \mathbb{R}^3 \,|\, f(x,y,z):= \big ( \sqrt{x^2+y^2} - R\big)^2 + z^2 - r^2=0 \big \}\,.
%%\]
%%That is, the torus consists of the points in $\mathbb{R}^3$ which have distance $r$ to a circle with
%%radius $R$. The defining function $f$ is continuously differentiable away from the $z$-axis.
%%However, when $r<R$ the torus does not contain any point on the $z$-axis. 
%%We calculate 
%%\[
%%Df(x,y,z)= \big ( 2 \big( \sqrt{x^2+y^2} - R\big) \frac{x}{  \sqrt{x^2+y^2}},
%%2 \big( \sqrt{x^2+y^2} - R\big) \frac{y}{  \sqrt{x^2+y^2}}, 2z \big) 
%%\]
%%and  see that $ Df(x,y,z)\not=0$ when $(x,y,z) \in T$.
%%Consequently, $\text{rank} Df(x,y,z)=1$ for $(x,y,z) \in T$ and 
%% we conclude that $T$ is a two-dimensional submanifold of $\mathbb{R}^3$.
%%
%%\item[e)] (Orthogonal group)
%%
%%We claim that 
%%    \[O(n) = \Big \{ X \in M_{n\times n}(\mathbb R) \mid X^T X = \text{Id}\Big \}\]
%%    is a submanifold of $\mathbb{R}^{n^2}$ of dimension
%%     ${1 \over 2} n (n-1)$.
%%
%%
%%    To see this let
%%    \[S(n) = \Big \{ X \in M_{n\times n}(\mathbb R) \mid X^T = X\Big \}\]
%%    be the set of symmetric matrices. 
%%    $S(n)$ is isomorphic to
%%    $\mathbb R^r$ with  $r= n + (n-1) + (n-2) + \ldots + 1 = {n (n+1) \over 2}$. 
%%
%%    Let
%%     $f\colon M_{n\times n}(\mathbb R) \rightarrow S(n)$ 
%%     be defined via
%%    \[f(X) = X^T  X \, .\]
%%    Then $O(n) = f^{-1} \{ \text{Id} \}$  and we need to identify the range of $df(x)$. 
%%
%%    We have for all  $H \in M_{n \times n}(\mathbb R)$  that
%%    \[d f(X) H = H^T X + X^T H \in S(n).\]
%%    It remains to show that for all 
%%     $X \in O(n)$ the map  $d f(X)$ is surjective.
%%     Let 
%%     $Z \in S(n)$ and define $H: = {1 \over 2} XZ$. Then
%%      \[d f(X) H  =  {1 \over 2} Z^T X^T X + {1 \over 2} X^T X Z
%%       =  {1 \over 2} (Z^T + Z)
%%       =  Z\,.
%%       \]
%%    Hence the range of $df(X)$ is $ S(n)$, thus
%%    $\text{rank} d f(X) = \dim S(n) = {1 \over 2} n (n+1)$ and 
%%    $O(n)$ is a submanifold of dimension
%%     $k = n^2 - {1 \over 2} n (n+1) = {1 \over 2} n (n-1)$.
%%  \end{itemize}
%%\end{example}
%%
%%\end{frame}
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}[fragile] \frametitle{}
%%The next proposition, which is a consequence of the Implicit
%%Function Theorem, will tell us that a submanifold can be locally
%%represented as graphs of functions. 
%%
%%\begin{block}{Proposition} (Submanifolds can be locally represented as  graphs)
%%  \label{P.graph}
%%
%%
%%  For a set $M \subseteq \mathbb R^n$ 
%%  the following properties are equivalent.
%%  \begin{itemize}
%%  \item[(1)] $M$ is a $k$-dimensional submanifold of $\mathbb{R}^n$ (in the sense of
%%  definition \ref{D.submanifold}).
%%  \item[(2)] For each $x \in M$ we can, after suitably relabelling the coordinates,
%%  write $x=(z_0,y_0)$ with $z_0 \in \mathbb{R}^k$, $y_0 \in \mathbb{R}^{n-k}$ and 
%%  find an  open neighbourhood
%%    $U$ of $z_0$ in $\mathbb{R}^k$, an open neighbourhood $V$ of $y_0$ in 
%%    $\mathbb R^{n-k}$, 
%%    and a map 
%%     $g \in C^1(U,V)$ with  $g(z_0) = y_0$, 
%%     such that 
%%    \[M \cap (U \times V) = \{ (z, g(z)) \mid z \in U \}.\]
%%  \end{itemize}
%%\end{block}
%%\end{frame}
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}[fragile] \frametitle{}
%%\begin{block}{Remark}
%%In (2) it is important that we remember that the statement is true only after
%%relabelling the coordinates. For instance, consider again the unit circle 
%%$S^1=\big \{ (x_1,x_2) \,|\, x_1^2 + x_2^2 =1 \big \}$ in 
%%$\mathbb{R}^2$. If $x=(1,0) \in S^1$ we have
%%\[
%%S^1 \cap \big ((0,2)\times (-1,1)\big) = \big \{ (\sqrt{1-z^2},z) \,|\, |z|<1 \big \}\,.
%%\]
%%Hence, to get the statement in (2) we have to relabel $(x_1,x_2)$ as $(x_2,x_1)$.
%%\end{block}
%%
%%\begin{proof}
%%  We first show that (1) implies (2):
%%  After possibly relabelling the coordinates  we can write $x$ as
%%  $x= (z_0,y_0)$  such that 
%%    $D_y f(x)$ is invertible.
%%    Then property (2) follows from the Implicit Function Theorem.
%%
%%    Now assume that (2) is satisfied. Define
%% $\Omega = U \times V$
%%    and  $f \in C^1(\Omega, \mathbb R^{n-k})$  via
%%    \[f(z,y) := y - g(z)\]
%%    Then $M \cap \Omega = f^{-1} \{ 0 \}$ 
%%    and $D f(z,y) = (-D g(z), \text{Id}_{n-k})$. It follows that
%%   $\text{rank} D f(z,y) = n-k$.
%%\end{proof}
%%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}[fragile] \frametitle{}
%%We now define an important concept for manifolds, the tangent space in a point of the manifold.
%%
%%\begin{definition} (Tangent vector, tangent space, normal vector)
%%
%%Let
%%  $M \subseteq \mathbb R^n$ 
%%  be a  $k$-dimensional submanifold of  $\mathbb R^n$.
%%  \begin{enumerate}
%%  \item We call $v \in \mathbb R^n$ a  \emph{tangent vector} to $M$ at $x \in M$, 
%%  if there exists a  $C^1$-function
%%     $\gamma\colon (-\varepsilon, \varepsilon) \rightarrow \mathbb{R}^n$,
%%    such that $\gamma(t) \in M$ for all $t \in (-\varepsilon, \varepsilon)$, 
%%    $\gamma(0) = x$ and $\gamma'(0) = v$.
%%  \item The set of all tangent vectors to  $M$ at $x$ 
%%  is called the 
%%    \emph{tangent space} to $M$ at $x$, 
%%    and we denote it by $T_x M$.
%%    \item We call $w \in \mathbb{R}^n$ a normal vector to $M$ at $x \in M$ if $\langle w,v \rangle=0$
%%    for all $v \in T_xM$. Thus the set of all normal vectors to $M$ at $x$ is precisely the orthogonal
%%    complement $T_xM^{\perp}$ of $T_xM$ in $\mathbb{R}^n$.
%%  \end{enumerate}
%%\end{definition}
%%
%%
%%Next we prove the generalisation of the property
%%that 'the gradient is perpendicular to the level sets of a function'.
%%This result in particular also shows that $T_xM$ is indeed  a $k$-dimensional  vector space 
%%and as a consequence that the space of normal vectors is an $(n-k)$-dimensional vector space.
%%\end{frame}
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}[fragile] \frametitle{}
%%
%%\begin{block}{Proposition}
%%\label{P.kernel}
%%
%%Let $M$ be a $k$-dimensional submanifold of $\mathbb{R}^n$. Let $\Omega$ be an
%%open subset of $\mathbb{R}^n$ and let $f \in C^1(\Omega, \mathbb{R}^{n-k})$
%%be such that $M \cap \Omega = f^{-1} \{0\}$ and $\text{rank} Df(x) = n-k$ for all $x \in \Omega$. Then we have 
%%  \[T_x M = \ker D f(x)  ,
%%  \]
%% for all $x \in M \cap \Omega$, that is the tangent space equals the kernel of $Df(x)$.
%%\end{block}
%%
%%
%%\begin{proof} 
%% We first  claim that  $T_x M \subseteq \ker D f(x)$:
%%
%%  Indeed, let 
%%     $ v \in T_x M$, 
%%    then there exists $\gamma: (-\varepsilon, \varepsilon) \rightarrow M$
%%    such that 
%%    \[\gamma(0) = x \quad \text{and} \quad \gamma'(0) = v.\]
%%    It follows for all
%%     $t \in (-\varepsilon, \varepsilon)$, 
%%    that $f(\gamma(t)) = 0$. Hence
%%    \[0 = {\dop\over \dt} f(\gamma(t)) = D f(\gamma(t)) \gamma'(t)\]
%%    and for  $t=0$ we find $0 = D f(x)  v$,  hence
%%     $v \in \ker D f(x)$.
%%
%%     Now recall that, possibly after a suitable relabelling, we can assume
%%     in view of   Proposition \ref{P.graph} that $x=(z_0,y_0)\in \mathbb{R}^k \times \mathbb{R}^{n-k}$
%%     and that there exist open
%%     subsets
%%          $U \subseteq \mathbb R^k $ with $z_0 \in U$ and $ V \subseteq \mathbb R^{n-k}$
%%	        with $y_0 \in V$    and a function  $g \in C^1(U,V)$
%%		with $g(z_0)=y_0$ 
%%		                such that
%%				                 \[M \cap (U \times V) = \{ (z, g(z)) \mid z \in U \}
%%						                      .\]
%%
%%
%%
%%    We define $G\colon U \rightarrow \mathbb R^n$ by $G(z) = (z, g(z))$
%%    and for an arbitrary  $\xi \in \mathbb R^k$ and sufficiently small $\varepsilon$ we let
%%    $\gamma\colon (-\varepsilon, \varepsilon) \rightarrow M$  be given by
%%    \[\gamma(t) :=  G(z_0+t \xi)\,.\]
%%   Then $\gamma'(t) = D G(z_0+t\xi)\xi$ and
%%    \[\gamma'(0) = D G(z_0) \xi
%%    \quad \text{ with } D G(z_0) = 
%%    \left ( \begin{array}{c} \text{Id}_k\\ Dg(z_0) \end{array} \right).\]
%%    Hence $\text{im} DG(z_0) \subseteq T_xM$ and thus we have shown so far that
%%    $\text{im} DG(z_0) \subseteq T_xM \subseteq \text{ker}Df(x)$. But $DG(z_0)$ is obviously 
%%    injective, hence $\text{ dim im}Dg(z_0)=k=n-\text{rank} Df(x) = \text{ dim ker}Df(x)$.
%%    Hence $\text{im} Dg(z_0) = \text{ker}Df(x) = T_xM$.
%%\end{proof}
%%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}[fragile] \frametitle{}
%%
%%\begin{example} \
%%  \begin{itemize}
%%  \item[a)]
%%
%%  Let $A \in M_{n\times n}(\mathbb{R})$ be 
%%  symmetric, $\text{det}A \not=0$, 
%%  and let $M = \{ x \in \mathbb R^n \mid f(x):=\left< x, Ax \right> - 1 = 0 \}$.
%%  We have
%%     $ D f(x) = 2 (Ax)^T$ and since $A$ is regular and $x \not=0$ for $x\in M$
%%     we have $\text{rank}Df(x)=1$ for all $x \in M$. Hence $M$ is an $(n-1)$-dimensional
%%     submanifold of $\mathbb{R}^n$ and Proposition \ref{P.kernel} implies that
%%    \[T_x M = \{ v \in \mathbb R^n \mid 2 \left <v, Ax \right> = 0 \}\]
%%    and the space of normal vectors is spanned by $Ax$.
%%
%%In particular, if $A=\text{Id}$, then $M$ is the unit sphere in $\mathbb{R}^n$,  the space of normal vectors at $x$
%%is spanned by $x$ and the tangent space  is given by all vectors, which are perpendicular to $x$.
%%  \item [b)]
%%  We have seen that a (two-dimensional) torus (in $\mathbb{R}^3$) is given by 
%%  \[
%%  T:=\big \{ (x,y,z) \in \mathbb{R}^3 \,|\, f(x,y,z):= \big ( \sqrt{x^2+y^2} - R\big)^2 + z^2 - r^2=0 \big \}\,,
%%  \]
%%  where $0<r<R$.
%%  Hence, the space of normal vectors is 
%%  \[
%% (T_{(x,y,z)}M)^{\perp}
%% = \big \{ w \in \mathbb{R}^3 \, |\, w = \lambda \nabla f(x,y,z)\, \text{ for some } \lambda \in \mathbb{R} \big\}\,.
%%  \]
%%
%%  
%%  \item[c)]
%%  Consider the orthogonal group
%%    \[O(n) = \Big \{ X \in M_{n\times n}(\mathbb R) \mid f(X) := X^T X - \text{Id} =0 \Big \}\,.
%%    \]
%%    We have seen that $O(n)$ is a submanifold of $\mathbb{R}^{n^2}$ of dimension $\frac 1 2 n (n-1)$
%%    and we also have $\text{Id}  \in O(n)$. 
%%    With $d f(X)  H = X^T H + H^T X$ and $ d f(\text{Id})  H = H^T + H$  it follows
%%    \[T_{\text{Id}} M = \{ H \in M_{n \times n}(\mathbb R) \mid H^T + H = 0 \},\]
%%    that is the tangent space in Id are the skew-symmetric matrices.
%%
%%  \end{itemize}
%%
%%\end{example}
%%\end{frame}
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}[fragile] \frametitle{Extremal problems with constraints }
%%
%%
%%
%%We consider now a real-valued function $g$ and are interested in finding extremal values of $g$
%%on a submanifold $M$. The following Theorem will provide a necessary condition in an extremal point.
%%
%%
%%
%%\begin{theorem} (Theorem on Lagrange multipliers)
%%\label{T.multipliers}
%%
%%  Let  $\Omega \subseteq \mathbb R^n$ be open,  
%%  $g \in C^1(\Omega)$ and $f \in C^1(\Omega, \mathbb R^{n-k})$.
%%  If   $x_0 \in f^{-1} \{ 0 \}$ is a local extremum of $g$ on $f^{-1} \{ 0 \}$, that is  
%%  there exists
%%    an open neighbourhood $V$ of $x_0$ such that for all 
%%     $x \in V$ which satisfy $f(x)=0$ 
%%     we have
%%    \[
%%    g(x) \geq  g(x_0)  \qquad (\text{or }   g(x) \leq g(x_0) ),
%%    \]
%%    and if
%% $\text{\rm rank} D f(x_0) = n-k$,
%%  then there exist $\lambda_1, \ldots, \lambda_{n-k} \in \mathbb{R}$, 
%%  such that
%%  \[ \nabla g(x_0) = \sum_{i=1}^{n-k} \lambda_i \nabla f_i(x_0). \]
%%  The numbers  $\lambda_1, \ldots, \lambda_{n-k}$ 
%%  are called  \emph{Lagrange multipliers}.
%%\end{theorem}
%%
%%\begin{proof}
%%If  $V$  is sufficiently small then we have for all 
%%$x \in V$ that 
%%  $\text{rank} D f(x) = n-k$,
%%  hence $M := f^{-1} \{ 0 \} \cap V$ is a  $k$-dimensional submanifold
%%  of $\mathbb{R}^n$. For a $v \in T_{x_0}M$ let 
%%   $\gamma: (-\varepsilon, \varepsilon) \rightarrow M$  be a $C^1$-function
%%  such that  $\gamma(0) = x_0$ and $\gamma'(0)=v$.
%%  The function $g \circ \gamma$ 
%%  has in  $t=0$ a local minimum. Thus 
%%  \[0 = {\dop\over \dt} g(\gamma(t)) |_{t=0}
%%  = \left< \nabla g(\gamma(t)), \gamma'(t) \right>|_{t=0}
%%  = \left< \nabla g(x_0) , v \right>\]
%%  and thus $\nabla g(x_0) \in (T_{x_0} M)^{\perp}$.
%%  Furthermore we have for all
%%   $x \in M$ and $i = 1, \ldots, n-k$  that
%%  \[f_i(x) = 0,
%%  \quad \text{ and thus in particular } \nabla f_i(x_0) \perp T_{x_0} M.\]
%%  Since $\text{rank} D f(x_0) = n-k$
%%  the vectors  $\nabla f_i(x_0)$  are linearly independent and form a basis of
%%   $(T_{x_0} M)^{\perp}$.
%%   Hence there exist $\lambda_1, \ldots, \lambda_{n-k}$ such that $\nabla g(x_0)= \sum_{i=1}^{n-k}
%%   \lambda_i \nabla f_i(x_0)$.
%%\end{proof}
%%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}[fragile] \frametitle{}
%%\begin{example}\
%%\begin{itemize}
%%\item[a)] We want to determine the minimal and maximal value of $g(x,y,z)=5x + y - 3z$ on 
%%the intersection of the plane $E=\{ (x,y,z) \in \mathbb{R}^3 \,|\, x+y+z=0\}$
%%with the unit sphere $S^2=\{ (x,y,z) \in \mathbb{R}^3 \,|\, x^2+y^2+z^2-1=0\}$.
%%
%%In other words we want to determine the extremal values of $g$ on
%%\[
%%M:=\Big \{ (x,y,z) \in \mathbb{R}^3 \,|\,
%%f(x,y,z) = \left ( \begin{array}{c} x+y+z\\ x^2+y^2+z^2-1 \end{array} \right) =0 \Big\}\,.
%%\]
%%We compute 
%%\[
%%Df(x,y,z) = \left ( \begin{array}{c c c} 1 & 1& 1\\ 2x &2y&2z \end{array} \right)
%%\]
%%and conclude that $\text{rank} Df(x,y,z)=2$ for all $(x,y,z) \in M$
%%(as $(1,1,1) \notin M$). Hence $M$ is a one-dimensional submanifold
%%of $\mathbb{R}^3$. Furthermore $M$ is compact, $g$ is continuous and hence $g$ attains its infimum and
%%supremum on $M$. Theorem \ref{T.multipliers}
%%implies that there exist $\lambda_1, \lambda_2 \in \mathbb{R}$  such that
%%in an extremal     point $(x,y,z)$ we have
%%$\nabla g(x,y,z) = \lambda_1 \nabla f_1(x,y,z) + \lambda_2 \nabla f_2(x,y,z)$. 
%%Thus, we find for an extremal point $(x,y,z)$ that
%%\[
%%\left ( \begin{array}{c} 5\\1\\-3 \end{array} \right) = \lambda_1
%%\left ( \begin{array}{c} 1\\1\\1 \end{array} \right) + \lambda_2
%%\left ( \begin{array}{c} 2x\\2y\\2z \end{array} \right)
%%\]
%%and $f(x,y,z)=0$. This implies that
%%\begin{equation}\label{eq1}
%%\lambda_1= 5 - \lambda_2 2x = 1 - \lambda_2 2y= -3 - \lambda_2 2z
%%\end{equation}
%%and thus
%%(since $\lambda_2=0$ is excluded)
%%\[
%%x=y+ 2/\lambda_2 \qquad \text{ and } \qquad z=y-2/\lambda_2\,.
%%\]
%%Next we conclude from $f_1(x,y,z)=0$ that $y=0$ and hence
%%$x=2/\lambda_2$ and $z=-2/\lambda_2$. Then it follows from $f_2(x,y,z)=0$ that $\lambda_2 = \pm 2 \sqrt{2}$. Thus
%% $g$ attains its maximum, which is $4 \sqrt{2}$ in $(2^{-1/2}, 0 , - 2^{-1/2})$ and its minimum,
%%which is $- 4\sqrt{2}$ in $(-2^{-1/2}, 0 ,  2^{-1/2})$.
%%
%%  \item[b)] (Inequality between geometric and arithmetic mean)
%%  
%%  Let $g\colon \mathbb{R}^n \to \mathbb{R}$ be given by 
%% $g(x) := \Pi_{i=1}^n x_i$.
%% Consider the set 
%%    \[M = \left\{ x \in \mathbb R^n 
%%      \mid f(x) := \big(\sum_{i=1}^n x_i\big) - 1 = 0, x_i > 0 \right\}\,,\]
%%      which obviously defines an  $(n-1)$-dimensional submanifold of $\mathbb{R}^n$.
%%      Since $\overline{M}$ is compact and $g$ is continuous, $g$ attains its supremum 
%%      in a point $x^0 \in \overline{M}$. Since $g=0$ on $\partial M$ and positive in the interior
%%      we conclude that indeed $x^0 \in M$.
%%      Now
%%      Theorem \ref{T.multipliers} implies that there exists $\lambda \in \mathbb{R}$ such that
%%    \[\nabla g(x^0) = \lambda \nabla f(x^0) = \lambda (1, \ldots, 1)^T.\]
%%    We find $\partial_i g(x)
%%    = x_1 \cdot \ldots \cdot x_{i-1} \cdot x_{i+1} \cdot \ldots \cdot x_n
%%    = {g(x) \over x_i}$ and hence 
%%      ${g(x^0) \over x^0_i} = \lambda$  for all $i=1, \ldots, n$.
%%      Consequently 
%%     $x_1^0 = x_2^0 = \ldots = x_n^0$ 
%%     and since
%%    $f(x^0) = 1$, 
%%    we have 
%%    $ x_i^0 = {1 \over n}$ for all $i=1, \ldots,n$.
%%Thus
%%    \begin{equation}\label{maximierer}
%%      g(x)  \leq g(x^0) = {1 \over n^n} \qquad \text{ for all } x \in M\,.
%%      \end{equation}
%%     Let now $a \in \mathbb R^n$ with $a_i > 0$ for  $1 \le i \le n$.
%%    With $\overline a := \sum_{i=1}^n a_i$ 
%%    we find $\left(
%%      {a_1 \over \overline a},  {a_2 \over \overline a}, \ldots,
%%      {a_n \over \overline a}, 
%%    \right) 
%%    \in M$.
%%    Equation (\ref{maximierer}) implies
%%    \[ {a_1 \cdot \ldots \cdot a_n \over \overline a^n}
%%    \le {1 \over n^n}, \]
%%    or
%%    \[ (a_1 \cdot \ldots \cdot a_n)^{1 \over n}
%%      \le {a_1 + \ldots + a_n \over n},\]
%%      which is the desired inequality between geometric and arithmetic mean.
%%  \item[c)] (Eigenvalues of symmetric real matrices)
%%  
%%  Let $A \in M_{n\times n}(\mathbb R)$ be symmetric  and let
%%  $g \colon \mathbb{R}^n \to \mathbb{R}$ 
%%  be given by 
%%    \[
%%    g(x) = \left< x, Ax \right>
%%    \quad \text{und} \quad
%%    M = \{ x \in\mathbb{R}^n \,|\, f(x) = |x|^2 - 1 = 0 \}.
%%    \]
%%    $M$ is an $(n-1)$-dimensional submanifold of $\mathbb{R}^n$ since for all 
%%     $x \in M$  we have
%%    \[\nabla f(x) = 2x \neq 0.\]
%%    $M$ is compact, so that  $g$ attains its supremum on   $M$ in a point $x_0$. 
%%    By Theorem 
%%   \ref{T.multipliers} there exists   $\lambda \in \mathbb R$, 
%%   such that
%%      $\nabla g(x_0)  =  \lambda \nabla f(x_0)$ that is
%%       $A x_0 = \lambda x_0$.
%%      This implies that $x_0$ is an eigenvector of $A$ with eigenvalue $\lambda$. 
%%      Thus, we have shown that every real symmetric $n \times n$ matrix 
%%      has a real eigenvalue. We also find that
%%    \[\lambda \left<x_0, x_0\right> = \left<x_0, A x_0 \right> = g(x_0) \qquad \text{ and thus }
%%    \; \lambda = g(x_0)\,. \]
%%    Since $g(x_0)$ is the maximal value of $g$ on $M$ this also implies that
%%    $\lambda$ is the largest eigenvalue of $A$.
%%  \end{itemize}
%%
%%\end{example}
%%\end{frame}