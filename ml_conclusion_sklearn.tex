%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Conculsion}
\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Recipe Tour}
Here, you will see 5 recipes of supervised classification algorithms applied to small standard datasets that are provided with the scikit-learn library. Each example is:
\begin{itemize}
\item Standalone: Each code example is a self-contained, complete and executable recipe.
\item Just Code: The focus of each recipe is on the code with minimal exposition on machine learning theory.
\item Simple: Recipes present the common use case, which is probably what you are looking to do.
\item Consistent: All code example are presented consistently and follow the same code pattern and style conventions.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Logistic Regression}
Logistic regression fits a logistic model to data and makes predictions about the probability of an event (between 0 and 1). 
\begin{lstlisting}
from sklearn import datasets
from sklearn import metrics
from sklearn.linear_model import LogisticRegression

dataset = datasets.load_iris()

model = LogisticRegression()
model.fit(dataset.data, dataset.target)
print(model)

expected = dataset.target
predicted = model.predict(dataset.data)

print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Naive Bayes}
Naive Bayes uses Bayes Theorem to model the conditional relationship of each attribute to the class variable.
This recipe shows the fitting of an Naive Bayes model to the iris dataset.
\begin{lstlisting}
from sklearn import datasets
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB

dataset = datasets.load_iris()

model = GaussianNB()
model.fit(dataset.data, dataset.target)
print(model)

expected = dataset.target
predicted = model.predict(dataset.data)

print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
\end{lstlisting}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{k-Nearest Neighbor}
The k-Nearest Neighbor (kNN) method makes predictions by locating similar cases to a given data instance (using a similarity function) and returning the average or majority of the most similar data instances. 
\begin{lstlisting}
from sklearn import datasets
from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier

dataset = datasets.load_iris()

model = KNeighborsClassifier()
model.fit(dataset.data, dataset.target)
print(model)

expected = dataset.target
predicted = model.predict(dataset.data)

print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Classification and Regression Trees}
Classification and Regression Trees (CART) are constructed from a dataset by making splits that best separate the data for the classes or predictions being made. 
\begin{lstlisting}
from sklearn import datasets
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier

dataset = datasets.load_iris()

model = DecisionTreeClassifier()
model.fit(dataset.data, dataset.target)
print(model)

expected = dataset.target
predicted = model.predict(dataset.data)

print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Support Vector Machines}
Support Vector Machines (SVM) are a method that uses points in a transformed problem space that best separate classes into two groups. 
\begin{lstlisting}
from sklearn import datasets
from sklearn import metrics
from sklearn.svm import SVC

dataset = datasets.load_iris()

model = SVC()
model.fit(dataset.data, dataset.target)
print(model)

expected = dataset.target
predicted = model.predict(dataset.data)

print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Benefits and drawbacks of scikit-learn}
Benefits
\begin{itemize}
\item Consistent interface to machine learning models
\item Provides many tuning parameters but with sensible defaults
\item Exceptional documentation
\item Rich set of functionality for companion tasks
\item Active community for development and support
\end{itemize}
Potential drawbacks
\begin{itemize}
\item Harder (than R) to get started with machine learning
\item Less emphasis (than R) on model interpret-ability
\end{itemize}
\end{frame}








